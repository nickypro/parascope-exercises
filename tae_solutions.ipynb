{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eefbac-976c-422d-9c2c-9bf526b25978",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Section 1: Text Autoencoders - Exploring SONAR\n",
    " This notebook explores Meta's SONAR text autoencoder, which can encode text\n",
    " into fixed-size vectors and decode them back to (approximately) the original text.\n",
    " Learning objectives:\n",
    " 1. Load and use SONAR for text encoding/decoding\n",
    " 2. Understand the properties of text embeddings\n",
    " 3. Test robustness to noise\n",
    " 4. Explore how text length affects embeddings\n",
    " 5. Experiment with token swapping and sentence combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4f67e-0109-47ea-a442-d843bed1cc50",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Setup and Installation\n",
    "\n",
    " First, we need to install SONAR and its dependencies. Just run, nothing worth reading here unless you get errors.\n",
    " Note: You may need to adjust the CUDA version in fairseq2 installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772feac0-5fca-440c-8a55-635576a6ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "!pip install -q fairseq2==0.4.5 sonar-space==0.4.0 torchvision==0.21.0 torch==2.6.0 torchaudio==2.6.0 plotly nbformat\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from jaxtyping import Float\n",
    "\n",
    "# Check if CUDA is available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device(DEVICE)\n",
    "torch.set_grad_enabled(False)  # We're only doing inference\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4ee27-025f-4c56-8af7-4d94b947f906",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Loading SONAR Models\n",
    "\n",
    " SONAR (Sentence-Level Multimodal and Language-Agnostic Representations) is Meta's text autoencoder\n",
    " that can encode entire sentences/paragraphs into fixed-size vectors and decode them back to approximately\n",
    " the original text.\n",
    "\n",
    " **What are Text Autoencoders?**\n",
    "\n",
    " Text Autoencoders are models that compress entire input sequences (sentences/paragraphs) into a single\n",
    " fixed-size vector representation (the \"bottleneck\"), then reconstruct the original text from that vector.\n",
    " Unlike typical text embedding models that only encode, these models have both an encoder AND decoder.\n",
    "\n",
    " ![Text Autoencoder Architecture](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db8d350884974ce6dcb1281011c5053e11b65711c12a4556.png)\n",
    "\n",
    " **How Text Autoencoders Work:**\n",
    " 1. **Encoder**: Takes input text → processes through Transformer → outputs single fixed-size vector (1024-dim)\n",
    " 2. **Bottleneck**: The compressed representation that captures semantic meaning in a dense vector\n",
    " 3. **Decoder**: Takes the vector → generates text that approximates the original input\n",
    "\n",
    " **Key Properties:**\n",
    " - **Lossy compression**: Some information is lost, but semantic meaning is preserved\n",
    " - **Fixed-size representation**: Any length text becomes same-size vector (useful for comparison/clustering)\n",
    " - **Cross-lingual**: Can encode in one language and decode in another\n",
    " - **Reconstruction capability**: Unlike embedding-only models, you can decode back to text\n",
    " - **Semantic preservation**: The bottleneck captures core meaning even with compression\n",
    "\n",
    " **SONAR Specifically:**\n",
    " - Trained on ~100B tokens with denoising and translation objectives\n",
    " - Uses 24-layer Transformer encoder and decoder, with mean-pooling to create the bottleneck vector\n",
    " - Supports 200+ languages and can handle up to 512 tokens of context\n",
    " - Currently one of the best-performing text autoencoders available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d2d89-fd17-4e48-ae30-8419cafd63ec",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " We start by loading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f8761-b726-42ea-9567-6b46329eb89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SONAR models...\n",
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SONAR models...\")\n",
    "text2vec = TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=DEVICE\n",
    ")\n",
    "vec2text = EmbeddingToTextModelPipeline(\n",
    "    decoder=\"text_sonar_basic_decoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=DEVICE\n",
    ")\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1478648-794c-41c5-b68d-2aeb5f4e4df2",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Basic Usage - Encoding and Decoding\n",
    "\n",
    " Test basic encoding and decoding functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c128205-6a81-4351-b111-93232b259d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2, 1024])\n",
      "Embedding dimension: 1024\n",
      "L2 norm of embeddings: [0.2274099737405777, 0.2351522445678711]\n",
      "\n",
      "Reconstruction quality:\n",
      "Original:      My name is SONAR.\n",
      "Reconstructed: My name is SONAR.\n",
      "\n",
      "Original:      I can embed sentences into vectorial space.\n",
      "Reconstructed: I can embed sentences into vector space.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Simple example sentences\n",
    "sentences = [\n",
    "    'My name is SONAR.',\n",
    "    'I can embed sentences into vectorial space.'\n",
    "]\n",
    "\n",
    "# Encode sentences to vectors\n",
    "embeddings = text2vec.predict(sentences, source_lang=\"eng_Latn\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # Should be [2, 1024]\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"L2 norm of embeddings: {torch.norm(embeddings, dim=1).tolist()}\")\n",
    "\n",
    "# Decode vectors back to text\n",
    "reconstructed = vec2text.predict(embeddings, target_lang=\"eng_Latn\", max_seq_len=512)\n",
    "print(\"\\nReconstruction quality:\")\n",
    "for orig, rec in zip(sentences, reconstructed):\n",
    "    print(f\"Original:      {orig}\")\n",
    "    print(f\"Reconstructed: {rec}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55337e21-8703-4b20-b3e3-42fe95b15597",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 1: Testing with Longer, More Realistic Text\n",
    " Let's test how well SONAR handles paragraph-length text.\n",
    "\n",
    " Write a function to reconstruct text from SONAR embeddings, and try testing with some longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c62f7-2bb6-4f7c-a381-fbde9ce03a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph reconstruction:\n",
      "\n",
      "--- Paragraph 1 ---\n",
      "Original (342 chars):\n",
      "SONAR is a model from August 2023, trained as a semantic text auto-encoder,\n",
      "converting text into sem...\n",
      "\n",
      "Reconstructed (358 chars):\n",
      "SONAR is a model from August 2023, which is trained as a semantic text auto-encoder, converting text...\n",
      "\n",
      "--- Paragraph 2 ---\n",
      "Original (236 chars):\n",
      "I tried it, and SONAR seems to work surprisingly well. For example, the above\n",
      "paragraph and this par...\n",
      "\n",
      "Reconstructed (231 chars):\n",
      "I tried it, and it seems SONAR works surprisingly well. For example, the above paragraph and this pa...\n",
      "\n",
      "--- Paragraph 3 ---\n",
      "Original (16 chars):\n",
      "Your text here.\n",
      "\n",
      "\n",
      "Reconstructed (15 chars):\n",
      "Your text here.\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_text(texts: list[str]) -> list[str]:\n",
    "    \"\"\"Reconstruct text from SONAR embedding, by first encoding and then decoding the text.\n",
    "\n",
    "    Args:\n",
    "        texts: List of strings to embed and then reconstruct.\n",
    "\n",
    "    Returns:\n",
    "        List of reconstructed strings.\n",
    "    \"\"\"\n",
    "    # [your implementation here]\n",
    "    embedding = text2vec.predict(texts, source_lang=\"eng_Latn\")\n",
    "    return vec2text.predict(embedding, target_lang=\"eng_Latn\", max_seq_len=512)\n",
    "\n",
    "# Longer example paragraphs\n",
    "paragraph1 = \"\"\"SONAR is a model from August 2023, trained as a semantic text auto-encoder,\n",
    "converting text into semantic embed vectors, which can later be decoded back into text.\n",
    "Additionally, the model is trained such that the semantic embed vectors are to some degree\n",
    "\"universal\" for different languages, and one can embed in French and decode in English.\"\"\"\n",
    "\n",
    "paragraph2 = \"\"\"I tried it, and SONAR seems to work surprisingly well. For example, the above\n",
    "paragraph and this paragraph, if each are encoded into two 1024 dimensional vectors\n",
    "(one for each paragraph), the model returns the following decoded outputs.\"\"\"\n",
    "\n",
    "paragraph3 = \"\"\"\\\n",
    "Your text here.\n",
    "\"\"\"\n",
    "\n",
    "# Test with paragraphs\n",
    "long_texts = [paragraph1, paragraph2, paragraph3]\n",
    "long_reconstructed = reconstruct_text(long_texts)\n",
    "\n",
    "print(\"Paragraph reconstruction:\")\n",
    "for i, (orig, rec) in enumerate(zip(long_texts, long_reconstructed)):\n",
    "    print(f\"\\n--- Paragraph {i+1} ---\")\n",
    "    print(f\"Original ({len(orig)} chars):\")\n",
    "    print(orig[:100] + \"...\" if len(orig) > 100 else orig)\n",
    "    print(f\"\\nReconstructed ({len(rec)} chars):\")\n",
    "    print(rec[:100] + \"...\" if len(rec) > 100 else rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f3f37-20db-42e9-ba9a-2d27ebccf1d3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " How well does it work for longer text? It should be doing a pretty good job. Bonus: How long does the text get before you see some degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91abb8-d18e-4f4a-a94a-5d0170c47b4a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 2: Noise Robustness Analysis\n",
    "\n",
    " In this exercise, we investigate SONAR's robustness to perturbations in the embedding space.\n",
    " We'll systematically add Gaussian noise of increasing magnitude to text embeddings and analyze\n",
    " how reconstruction quality degrades. This helps us understand:\n",
    " 1. How stable the embedding space is to small perturbations\n",
    " 2. The sensitivity of the decoder to different noise directions\n",
    "\n",
    " Write a function to test the robustness of SONAR to noise, and try it out with some different noise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ee60a-16d8-40e4-9677-ccddd23698e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 0.0\n",
      "Cosine similarity: 1.000\n",
      "Reconstructed: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 0.1\n",
      "Cosine similarity: 0.995\n",
      "Reconstructed: The fast brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 0.3\n",
      "Cosine similarity: 0.958\n",
      "Reconstructed: The fast brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 0.5\n",
      "Cosine similarity: 0.892\n",
      "Reconstructed: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 0.7\n",
      "Cosine similarity: 0.813\n",
      "Reconstructed: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 1.0\n",
      "Cosine similarity: 0.695\n",
      "Reconstructed: The fast brown fox jumps over the lazy dog.\n",
      "\n",
      "Noise scale: 1.5\n",
      "Cosine similarity: 0.548\n",
      "Reconstructed: The quick brown fox is able to get out and jumps over the lazy donkey.\n",
      "\n",
      "Noise scale: 2.0\n",
      "Cosine similarity: 0.457\n",
      "Reconstructed: The red fox is the fastest and the red fox is the slowest.\n",
      "\n",
      "Noise scale: 2.5\n",
      "Cosine similarity: 0.341\n",
      "Reconstructed: The Casual Brush Rush\n",
      "\n",
      "Noise scale: 3.0\n",
      "Cosine similarity: 0.295\n",
      "Reconstructed: there was a one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second, one-second.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_noise_robustness(text, noise_levels):\n",
    "    \"\"\"Test how reconstruction quality degrades with noise.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get original embedding\n",
    "    original_emb = text2vec.predict([text], source_lang=\"eng_Latn\")\n",
    "    original_norm = torch.norm(original_emb)\n",
    "\n",
    "    results = []\n",
    "    for noise_scale in noise_levels:\n",
    "        # [your implementation here]\n",
    "        # Add Gaussian noise\n",
    "        noise = torch.randn_like(original_emb)\n",
    "        noise = noise_scale * original_norm * noise / torch.norm(noise)\n",
    "        noisy_emb = original_emb + noise\n",
    "\n",
    "        # Decode noisy embedding\n",
    "        reconstructed = vec2text.predict(noisy_emb, target_lang=\"eng_Latn\", max_seq_len=512)[0]\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "            original_emb, noisy_emb, dim=1\n",
    "        ).item()\n",
    "\n",
    "        results.append({\n",
    "            'noise_scale': noise_scale,\n",
    "            'cosine_similarity': cosine_sim,\n",
    "            'reconstruction': reconstructed\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test with different noise levels\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "noise_levels = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "\n",
    "print(f\"Original text: {test_text}\\n\")\n",
    "results = test_noise_robustness(test_text, noise_levels)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Noise scale: {res['noise_scale']:.1f}\")\n",
    "    print(f\"Cosine similarity: {res['cosine_similarity']:.3f}\")\n",
    "    print(f\"Reconstructed: {res['reconstruction']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1a9c7-ecca-4b57-98aa-21a6493df0af",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " What do you see?\n",
    " It should be the case that with little noise, the reconstruction is still good. With more noise, the reconstruction gets worse. However, I found there is a lot of variance in the results, so try running it a few times. It seems like some directions have basically no effect, and others have a lot of effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89178b21-b4bb-45a5-a2f0-9374c2945ab5",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 3: Text Length vs Vector Norm Analysis\n",
    "\n",
    " ### Exercise 3: Investigating the Relationship Between Text Length and Embedding Norms\n",
    "\n",
    " In this exercise, we'll explore whether there's a correlation between the length of text\n",
    " and the L2 norm (magnitude) of its embedding vector. This analysis will help us understand:\n",
    " - How semantic information is distributed across embedding dimensions\n",
    " - Whether longer texts result in larger embedding magnitudes\n",
    " - If the embedding space has inherent biases based on text length\n",
    "\n",
    " We'll test this hypothesis using three different types of text:\n",
    " 1. Repeated words (to test pure length effects)\n",
    " 2. Random character sequences (to test meaningless content)\n",
    " 3. Natural language sentences (to test realistic content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fdc01-6900-4e0a-b0fe-c49f4e8227a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "word..."
          ],
          [
           "sentence..."
          ],
          [
           "paragraph..."
          ],
          [
           "dog..."
          ],
          [
           "spicy..."
          ],
          [
           "anime..."
          ],
          [
           "word word..."
          ],
          [
           "sentence sentence..."
          ],
          [
           "paragraph paragraph..."
          ],
          [
           "dog dog..."
          ],
          [
           "spicy spicy..."
          ],
          [
           "anime anime..."
          ],
          [
           "word word word..."
          ],
          [
           "sentence sentence sentence..."
          ],
          [
           "paragraph paragraph paragraph..."
          ],
          [
           "dog dog dog..."
          ],
          [
           "spicy spicy spicy..."
          ],
          [
           "anime anime anime..."
          ],
          [
           "word word word word..."
          ],
          [
           "sentence sentence sentence sentence..."
          ],
          [
           "paragraph paragraph paragraph paragraph..."
          ],
          [
           "dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy..."
          ],
          [
           "anime anime anime anime..."
          ],
          [
           "word word word word word..."
          ],
          [
           "sentence sentence sentence sentence sentence..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph..."
          ],
          [
           "dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy..."
          ],
          [
           "anime anime anime anime anime..."
          ],
          [
           "word word word word word word..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy..."
          ],
          [
           "anime anime anime anime anime anime..."
          ],
          [
           "word word word word word word word..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy..."
          ],
          [
           "anime anime anime anime anime anime anime..."
          ],
          [
           "word word word word word word word word..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy..."
          ],
          [
           "anime anime anime anime anime anime anime anime..."
          ],
          [
           "word word word word word word word word word..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ],
          [
           "word word word word word word word word word word ..."
          ],
          [
           "sentence sentence sentence sentence sentence sente..."
          ],
          [
           "paragraph paragraph paragraph paragraph paragraph ..."
          ],
          [
           "dog dog dog dog dog dog dog dog dog dog dog dog do..."
          ],
          [
           "spicy spicy spicy spicy spicy spicy spicy spicy sp..."
          ],
          [
           "anime anime anime anime anime anime anime anime an..."
          ]
         ],
         "hovertemplate": "type=Repeated Words<br>Text Length (characters)=%{x}<br>Embedding L2 Norm=%{y}<br>text_truncated=%{customdata[0]}<extra></extra>",
         "legendgroup": "Repeated Words",
         "marker": {
          "color": "#636efa",
          "opacity": 0.5,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Repeated Words",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "BAAIAAkAAwAFAAUACQARABMABwALAAsADgAaAB0ACwARABEAEwAjACcADwAXABcAGAAsADEAEwAdAB0AHQA1ADsAFwAjACMAIgA+AEUAGwApACkAJwBHAE8AHwAvAC8ALABQAFkAIwA1ADUAMQBZAGMAJwA7ADsANgBiAG0AKwBBAEEAOwBrAHcALwBHAEcAQAB0AIEAMwBNAE0ARQB9AIsANwBTAFMASgCGAJUAOwBZAFkATwCPAJ8APwBfAF8AVACYAKkAQwBlAGUAWQChALMARwBrAGsAXgCqAL0ASwBxAHEAYwCzAMcATwB3AHcAaAC8ANEAUwB9AH0AbQDFANsAVwCDAIMAcgDOAOUAWwCJAIkAdwDXAO8AXwCPAI8AfADgAPkAYwCVAJUAgQDpAAMBZwCbAJsAhgDyAA0BawChAKEAiwD7ABcBbwCnAKcAkAAEASEBcwCtAK0AlQANASsBdwCzALMAmgAWATUBewC5ALkAnwAfAT8BfwC/AL8ApAAoAUkBgwDFAMUAqQAxAVMBhwDLAMsArgA6AV0BiwDRANEAswBDAWcBjwDXANcAuABMAXEBkwDdAN0AvQBVAXsBlwDjAOMAwgBeAYUBmwDpAOkAxwBnAY8BnwDvAO8AzABwAZkBowD1APUA0QB5AaMBpwD7APsA1gCCAa0BqwABAQEB2wCLAbcBrwAHAQcB4ACUAcEBswANAQ0B5QCdAcsBtwATARMB6gCmAdUBuwAZARkB7wCvAd8BvwAfAR8B9AC4AekBwwAlASUB+QDBAfMBxwArASsB/gDKAf0BywAxATEBAwHTAQcCzwA3ATcBCAHcAREC0wA9AT0BDQHlARsC1wBDAUMBEgHuASUC2wBJAUkBFwH3AS8C3wBPAU8BHAEAAjkC4wBVAVUBIQEJAkMC5wBbAVsBJgESAk0C6wBhAWEBKwEbAlcC7wBnAWcBMAEkAmEC8wBtAW0BNQEtAmsC9wBzAXMBOgE2AnUC+wB5AXkBPwE/An8C/wB/AX8BRAFIAokCAwGFAYUBSQFRApMCBwGLAYsBTgFaAp0CCwGRAZEBUwFjAqcCDwGXAZcBWAFsArECEwGdAZ0BXQF1ArsCFwGjAaMBYgF+AsUCGwGpAakBZwGHAs8CHwGvAa8BbAGQAtkCIwG1AbUBcQGZAuMCJwG7AbsBdgGiAu0CKwHBAcEBewGrAvcCLwHHAccBgAG0AgEDMwHNAc0BhQG9AgsDNwHTAdMBigHGAhUDOwHZAdkBjwHPAh8DPwHfAd8BlAHYAikDQwHlAeUBmQHhAjMDRwHrAesBngHqAj0DSwHxAfEBowHzAkcDTwH3AfcBqAH8AlEDUwH9Af0BrQEFA1sDVwEDAgMCsgEOA2UDWwEJAgkCtwEXA28DXwEPAg8CvAEgA3kDYwEVAhUCwQEpA4MDZwEbAhsCxgEyA40DawEhAiECywE7A5cDbwEnAicC0AFEA6EDcwEtAi0C1QFNA6sDdwEzAjMC2gFWA7UDewE5AjkC3wFfA78DfwE/Aj8C5AFoA8kDgwFFAkUC6QFxA9MDhwFLAksC7gF6A90DiwFRAlEC",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "ctgdctop..."
          ],
          [
           "afn oss..."
          ],
          [
           "fpvaus hftcj wpusn..."
          ],
          [
           "chqxje bfh wwj qjjfgy..."
          ],
          [
           "bqngmhyr rvuf ukb wirkxlg totl..."
          ],
          [
           "kfzncbc erpkpmgo rcx tihea wgnexwhq fjeyxxp..."
          ],
          [
           "aywvhb ycmbttd yenpzy rskrjxlg ipf asbb wbg..."
          ],
          [
           "moswogmk cll wbqqbs gel emwbvw bzv ywe mwxeak..."
          ],
          [
           "hssurm hzpo ibmxnbnw prg yop qkp igrigdq itblzz nn..."
          ],
          [
           "wwjev hzunroyc gxte psfqg gxbqvb uqgto ocyrda pddi..."
          ],
          [
           "tygeuek tiyu vgucwwfv jvwfrk wzhqk iawgpza lsipnko..."
          ],
          [
           "okvhfup mlfmx ecnqivtr udbz wbkqfsm drxw ufu nbgwl..."
          ],
          [
           "fkimbvku awomyhkx ntuxmh airkem znxp aqdmbjfi hykv..."
          ],
          [
           "nrbqa vcyqvs enqqxu ltgwvc gmpjaw mjgh shaycsm rqm..."
          ],
          [
           "qtca mleby cbmprd ycgokpq jxhplob ctqgcgoe lphgss ..."
          ],
          [
           "wis rktac izknz wmbnqmlb oylsd pgeooczx yqeyhs vzg..."
          ],
          [
           "jistwab rhfcztsc sxbu jlgkr ecwq mpgq gauxre yzhzr..."
          ],
          [
           "zujuan jtgjqj gawoxfbv hkdyhmcx kjys ktekcmk ajlyw..."
          ],
          [
           "wqt wlxkvksi rjdxb cct opoi kwy wszpjp emwj jtouvz..."
          ],
          [
           "bwbax ooskc zey vxcvgp cwtfubq nxjsvvg gedt eum tl..."
          ],
          [
           "not aitdjev velkrgl tuggz xfrolu nkefpw qwo snd th..."
          ],
          [
           "zmedsy hbf iik maqhwbr djxtx ierjug scp oicr lsell..."
          ],
          [
           "vnh isiy vgqod pcqpp rspv bwcyugf kqmqhym klrekmid..."
          ],
          [
           "uupspeqq drayca kkfnp wtbdfi mjylnorw qspegofz usr..."
          ],
          [
           "ajlg hsaqs rme nxsidxhp ymlz komnr trpu lscobon ne..."
          ],
          [
           "jnskheng hsux kknkgic wcsegr bbuprff hxfet zmgpfxo..."
          ],
          [
           "vyhoowro tdzsnz cgb tykuahz fojepj hgbgjue icaijh ..."
          ],
          [
           "dyd iunjc sihvcwqr ytno oygke rktwllaf tvbcw jfcr ..."
          ],
          [
           "hyjyiaut zqswukzv gvipom xtbqv dwl zlz whww rgj ek..."
          ],
          [
           "bsocljm eeuxrct bwg jxeyau tkikshcj rbfnbkw uiclj ..."
          ],
          [
           "fxnquu ujomxn hapavh ntzzcr nutty wujvvep sxasyooe..."
          ],
          [
           "ktfbknz wmwmcxsn urqdtez vdlz fvdpis coo nxasp cdo..."
          ],
          [
           "kxjpm mwp bxnwjhb dfko dweciz qahte mtcp llmansyx ..."
          ],
          [
           "ntsfywnr crgirkv xpaynmm bmgz noj cloc ldmqck ovuz..."
          ],
          [
           "qrmojuif nueo cffijf nzvzarl pqyrixyk sanunnm xfny..."
          ],
          [
           "qvnjxnj mmpnd fnhd jsdf smmfe aboeqb dexnkzq svyfp..."
          ],
          [
           "mftw fvvj dcy ptobmq xwohet hfobumo jwuldni afp jw..."
          ],
          [
           "lacafkum muivy gko evvv zsk gzftcr mrii stodmhjb f..."
          ],
          [
           "twyq jhbodnfb zztoclud cobkc lstmfko qtgbuiy ckj i..."
          ],
          [
           "onq elw lkpbn mqcn uysd ialxxw grx sdcl csszp jjtx..."
          ],
          [
           "fakw ayr wixctta qffhn rhecyo fji yuoe pildm bjcks..."
          ],
          [
           "epdlvgs enazzp vtkj lqftiqtd mmmy grkgnlh jinycn y..."
          ],
          [
           "jewcays xkuusfq lgerpsp quj oez qwz furhfur mexbjf..."
          ],
          [
           "lznda tyyezo ismhyzo usxi txd daqfjip pfyh cwr zkm..."
          ],
          [
           "ltrvf hwebvm jarrknrd kpcyzb ipzd shsphu yjryzndl ..."
          ],
          [
           "wbnsc lrzwyjs azpvr rug kqsxrng kxkzgc hwel wtkaw ..."
          ],
          [
           "moktvp qcnzrq emqnhlxm zrhg hbbbcl msv gcncr uvbk ..."
          ],
          [
           "gvhkntv qjbqd oaigt oxqnpjh odjtcnfu csvcbx fcrkou..."
          ],
          [
           "jcphsgmj mspaz kwomoror eptdyww uerzsngo alrqwi hr..."
          ],
          [
           "hvrhs zttx uhj yvtarz wfsf nuts icvlyk lojpdy mrj ..."
          ],
          [
           "ilawbfq ydeo gcsi flphozfw czkeln leymtlcx pdsgm h..."
          ],
          [
           "rdhtwv piz hwo vmsjzivf yftyuwsh xeakkmjk nhs soci..."
          ],
          [
           "yynxy wbalhcj ejcmcndd augmjj nhzr ejljwf hdki efm..."
          ],
          [
           "jqdyx pen elxrqhw xryvs uwz ddluzvcl hczekils pqdy..."
          ],
          [
           "rqizytti xymi knalvdlz ate oejjyzoa axnws zkjvlziy..."
          ],
          [
           "mhumctsj rtfc bafvec vtni srwcd klqfk heh qdwtgxe ..."
          ],
          [
           "nwfxn brjsisu urybv jmroyu gspppvb wec mri zvtg sj..."
          ],
          [
           "pbsbsg avwob jcln fovr qwd pjzxj qebyx cfi unpifva..."
          ],
          [
           "aay nub hfugh vjk wwbnstat pdtufpi lthlc ojlfkhhs ..."
          ],
          [
           "avkn zlqr fsnvu nff acred iorxicj emi eje eavlolk ..."
          ],
          [
           "exjqzkqq pnco yigomc apukn wejw msfd xfl rlge ncpi..."
          ],
          [
           "xzdieo nsvlao qka lgfri qfvyxs dufeapv oyii rfez y..."
          ],
          [
           "sbqllw yvq raoayc uswgvi huis gufuoikj vwiq uzihxc..."
          ],
          [
           "gryknhoc kxf omd mcf etugpl ltrn tsba iyxj xbjy be..."
          ],
          [
           "ltmmt otfer mmywzm ldtkad jwdjl xdyfamhf khyofm ml..."
          ],
          [
           "diqtwozu kvwzonog refsidt okrkp ktcqlvxe xzu dbygk..."
          ],
          [
           "jwyr gslukij lllljg mewqp vzjt seq bcylhwh fing qo..."
          ],
          [
           "skuq ipicuzfd rtnnxzpp utbzph bikqum yxpgip gxwvrp..."
          ],
          [
           "jtnml abodkbm zbrzldt jaap zqx cypc unqtb hetsymzu..."
          ],
          [
           "ylztnc duabfznf kpvtkr itc tgib neomwl qdfngtb hou..."
          ],
          [
           "qqhlolv slj golm traqhuz srl njdqxns vdnf aqinkeqg..."
          ],
          [
           "fhh qvwocb ywoy adp ytrcfhwn pfa zcrmpc bddfdr ykt..."
          ],
          [
           "rkhbpkqv jggpal dldik zjtv hoazyu nnnpw zkdz lolqb..."
          ],
          [
           "ark jbavpw mkachjuh goyqqcnb hutodi wpvuaf ltx jvy..."
          ],
          [
           "ffic tab vbgqyo ngdxck uqax yyltz uoioeet ikgvku u..."
          ],
          [
           "dahz uufkusme ffq snkxa eswuiszj rxddsvfq pehwrvu ..."
          ],
          [
           "yevph iliwxe dzqg aioc wffg ltdxuj llu ptixbhj cxx..."
          ],
          [
           "dli pxeiprq ttx wagmtkp blqmo ulrmorw kbnuu cmhly ..."
          ],
          [
           "kgjubai zzvlj lgqwv poju chk prvxmeae ktjaxhn ugvr..."
          ],
          [
           "ykaj mrx fxqfiy ilac cuyoior quz ohwkw meb kbdgdc ..."
          ],
          [
           "dlxrd wsy szkk jdxl vpkragx zpgbijg jan umptyiw yu..."
          ],
          [
           "ska dmakkyt mld fnyao mfll rwrfwf cqugsz dfcjx dtl..."
          ],
          [
           "rexpyof twknsle adgenw xvcfn zped axh qrm dqtbut f..."
          ],
          [
           "mmtogaqo ajmf rpmu fzkg wxit yxlqpe lxqs swqb racr..."
          ],
          [
           "cwoeounk uctvavak tzadldmd doxfxny rplbsb inknsf i..."
          ],
          [
           "skfqyifr onlrau sdopgp uccwrnc rscgmfd mev yhp yxa..."
          ],
          [
           "ahjbzf durhefcq nfhx pkxr cemypva kot jge jfwvkz t..."
          ],
          [
           "kbprng uapje yzjsf qyevpt zslj mgbcm pcyasbkv kaak..."
          ],
          [
           "cvgu xpyl yiaiqp xdrs fuyyfu xnbnwch bjvk broug gm..."
          ],
          [
           "ghzwh zasgvjsh jxcrnr khpoc jqqbfb vdvzr bxewrvb f..."
          ],
          [
           "giqj yib vhl csxqxkp rilts thzh tepdywp dkryx azx ..."
          ],
          [
           "ajoqomlp irkscup ruyl rjflzj uspbbdnf zfi tpb bmwe..."
          ],
          [
           "eroge fsb xityufo nmxrf sabcglp dmgr ncqaj clpwmdq..."
          ],
          [
           "afyb tlwyec ntvcn ewwabkbo wfd oumppjf imp vwhhaez..."
          ],
          [
           "ubms felszth dadbl odm breplxqi qsjc ayxachng dwfb..."
          ],
          [
           "ydhkbp udksstg aiwbnl ksoaf vqjzzpjb pedetg rgjrck..."
          ],
          [
           "qnubqcb scvjhhdu urm twmaso yesrg yuthzasp gquage ..."
          ],
          [
           "wur vkllk hunnlpmp wuf ttble zffvkyg yjlg xtzlugt ..."
          ],
          [
           "sxixdurd yctkto aepdvk pqukqdj vtvh bjyy cyih iitv..."
          ]
         ],
         "hovertemplate": "type=Random Characters<br>Text Length (characters)=%{x}<br>Embedding L2 Norm=%{y}<br>text_truncated=%{customdata[0]}<extra></extra>",
         "legendgroup": "Random Characters",
         "marker": {
          "color": "#EF553B",
          "opacity": 0.5,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Random Characters",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "CAAHABIAFQAeACsAKwAtADMAOwBMAFAAXABZAGUAbgBxAH4AcQCOAIIAkwCTAJYAmwCvALAArwC7ALsA0ADfAMgA4wD1AP8A8wAAAfcA7wAQAfwAHgEQARoBJgE3AScBPgFGAUABWgFcAV0BdgFyAWABZQFxAW0BnwGUAZYBlQGiAb0BqwGuAdcBvwHBAc0B2QEDAtoB7wHtAfcBEQL3AewBDgIVAi4CMQITAkkCQwI/AlcCXAJtAl4CVgJ0Ao0CjAKhAmwC",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAMpn0T8AAAAAKrfOPwAAACAdJc4/AAAAIE0hzT8AAACAXULHPwAAAMAoPs0/AAAAwKBwzD8AAABg0CHRPwAAAODHa88/AAAAYHMGyT8AAABgGa7CPwAAAMC88MM/AAAAwBulxT8AAACArafIPwAAAACIJ8c/AAAAoJCwyT8AAACgssfJPwAAAACSn8Q/AAAAYCaRxD8AAADgRdrFPwAAAOA+TcU/AAAAwG/hwj8AAADg52rFPwAAAMC1dMk/AAAAIN0dxj8AAADgmQnDPwAAAIAbWsU/AAAAYMB7wz8AAABg9o3EPwAAAADT1cc/AAAAQHpXxT8AAADg80jFPwAAAKAtscM/AAAAQEuZwj8AAACAeAzEPwAAAOCywsM/AAAAwFS2xD8AAACgu+3FPwAAACCVq8Y/AAAAQD6PxD8AAAAg6crDPwAAAGCyn8Q/AAAAgDnwwj8AAABAXbbGPwAAAIBG6sU/AAAAgDpuxT8AAACA4vnHPwAAACCz5sU/AAAAgD2uxj8AAADAMarFPwAAAEB1L8Y/AAAAQODZxj8AAABAiKfHPwAAAIA/P8c/AAAAALAqxj8AAACgaLDIPwAAAIAhkMc/AAAAwCWRxz8AAAAgBADHPwAAAKAQQcc/AAAAYD3nyD8AAACgCevGPwAAAEAb1Mc/AAAAoPlBxz8AAABgJ4rHPwAAAODtHcg/AAAAQNr6xj8AAAAgMuLHPwAAAEDAY8o/AAAAAAZtyj8AAADgVhPIPwAAACDhnMk/AAAA4BrJyT8AAAAAh4HJPwAAAOCHOMg/AAAAIMTLyj8AAACAwIjKPwAAAGDk0Mk/AAAAoA/eyz8AAADgXyvKPwAAAEDqdsk/AAAAgKgwzD8AAACg+cvKPwAAAKC69cw/AAAAoKFjyj8AAADgErvKPwAAAABNxMw/AAAAoMJpzT8AAABAypPNPwAAAMAaxM0/AAAAYFOqzD8AAABgKebLPwAAAKBKdc0/AAAAIIPIzT8AAABAhH7OPwAAAKD0Qc4/AAAAwLJQzT8AAACg0NfOPwAAAMAit8w/",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Hi..."
          ],
          [
           "Hello..."
          ],
          [
           "Good morning..."
          ],
          [
           "Hello there..."
          ],
          [
           "How are you?..."
          ],
          [
           "Nice to meet you..."
          ],
          [
           "The cat sat on the mat..."
          ],
          [
           "I like to read books..."
          ],
          [
           "The weather is nice today..."
          ],
          [
           "She went to the store yesterday..."
          ],
          [
           "The quick brown fox jumps over the lazy dog..."
          ],
          [
           "I enjoy listening to music in the evening..."
          ],
          [
           "She sells seashells by the seashore on weekends..."
          ],
          [
           "To be or not to be, that is the question..."
          ],
          [
           "The early bird catches the worm every morning..."
          ],
          [
           "A picture is worth a thousand words in most cases..."
          ],
          [
           "Write a Chapter titled \"The Quiet Revolution of Ja..."
          ],
          [
           "**Chapter 7: The Quiet Revolution of Jane Austen's..."
          ],
          [
           "In an era where the French Revolution's loud decla..."
          ],
          [
           "Austen's writing career spanned the late 18th and ..."
          ],
          [
           "In _Pride and Prejudice_, Austen subverts the conv..."
          ],
          [
           "Similarly, in _Sense and Sensibility_, Austen expl..."
          ],
          [
           "Austen's advocacy for female independence was not ..."
          ],
          [
           "In contrast to the more explosive consequences of ..."
          ],
          [
           "Austen's legacy is a testament to the power of qui..."
          ],
          [
           "In the end, Austen's legacy serves as a reminder t..."
          ],
          [
           "Write a chapter titled \"The Importance of Play in ..."
          ],
          [
           "**Chapter 7: The Importance of Play in Childhood**..."
          ],
          [
           "Play has long been a cornerstone of childhood, yet..."
          ],
          [
           "**Biological Foundations of Play**\n\n..."
          ],
          [
           "Play is a universal aspect of human experience, fo..."
          ],
          [
           "During childhood, the brain undergoes rapid growth..."
          ],
          [
           "**The Role of Play in Neurological Growth and Deve..."
          ],
          [
           "Play has been shown to have a profound impact on n..."
          ],
          [
           "1. **Neuroplasticity**: Play stimulates the growth..."
          ],
          [
           "**Consequences of Play Deprivation**\n\n..."
          ],
          [
           "Play deprivation can have serious consequences for..."
          ],
          [
           "1. **Behavioral problems**: Play deprivation has b..."
          ],
          [
           "**The Ongoing Conversation about Play**\n\n..."
          ],
          [
           "In recent years, there has been a growing conversa..."
          ],
          [
           "Some argue that play is a luxury that can no longe..."
          ],
          [
           "**The Way Forward**\n\n..."
          ],
          [
           "As we move forward, it's essential to recognize th..."
          ],
          [
           "1. **Make time for play**: Prioritize playtime in ..."
          ],
          [
           "In conclusion, play is a vital component of childh..."
          ],
          [
           "Write a High-Level Guide, titled \"Understanding HI..."
          ],
          [
           "**Understanding HIV Transmission**\n\n..."
          ],
          [
           "HIV (Human Immunodeficiency Virus) is primarily sp..."
          ],
          [
           "Other factors that contribute to HIV transmission ..."
          ],
          [
           "It's also important to note that HIV can be transm..."
          ],
          [
           "Write a high-level overview of Microsoft's licensi..."
          ],
          [
           "**Microsoft Volume Licensing Overview**\n\n..."
          ],
          [
           "Microsoft offers a range of licensing options to h..."
          ],
          [
           "**Client Licensing**\n\n..."
          ],
          [
           "Microsoft offers various client licensing options ..."
          ],
          [
           "* **Volume Licensing**: Discounts for large organi..."
          ],
          [
           "**Public Domain and Copyleft**\n\n..."
          ],
          [
           "Microsoft software that is in the public domain or..."
          ],
          [
           "* **Apache Software Foundation (ASF) Software**: M..."
          ],
          [
           "**Shareware and Freeware**\n\n..."
          ],
          [
           "Microsoft offers shareware and freeware options fo..."
          ],
          [
           "* **Microsoft Office Online**: A free online versi..."
          ],
          [
           "**Server-Based Software Licensing**\n\n..."
          ],
          [
           "Microsoft offers various server-based software lic..."
          ],
          [
           "* **Server Licensing**: Discounts for large organi..."
          ],
          [
           "**Key Considerations**\n\n..."
          ],
          [
           "When evaluating Microsoft volume licensing options..."
          ],
          [
           "* **Business needs**: Determine the specific licen..."
          ],
          [
           "By understanding Microsoft's volume licensing opti..."
          ],
          [
           "Write a chapter, titled \"Revolutionary Desalinatio..."
          ],
          [
           "**Chapter 7: Revolutionary Desalination Membrane**..."
          ],
          [
           "The world's growing demand for freshwater has led ..."
          ],
          [
           "**Challenges in Traditional Desalination**\n\n..."
          ],
          [
           "Traditional desalination processes, which use reve..."
          ],
          [
           "Another challenge in traditional desalination is s..."
          ],
          [
           "**New Class of Reverse-Osmosis Membranes**\n\n..."
          ],
          [
           "To address these challenges, researchers have been..."
          ],
          [
           "Another key feature of these new membranes is thei..."
          ],
          [
           "**Benefits of the New Technology**\n\n..."
          ],
          [
           "The new class of reverse-osmosis membranes offers ..."
          ],
          [
           "1. **Improved fouling resistance**: The advanced m..."
          ],
          [
           "**Potential for Significant Cost Reduction**\n\n..."
          ],
          [
           "The new class of reverse-osmosis membranes has the..."
          ],
          [
           "1. **Reduced energy consumption**: Improved membra..."
          ],
          [
           "In conclusion, the development of a new class of r..."
          ],
          [
           "Write a News Article titled \"Leaves of Wonder\", wh..."
          ],
          [
           "**Leaves of Wonder**\n\n..."
          ],
          [
           "As a young reporter, I embarked on a thrilling adv..."
          ],
          [
           "Once I had collected my bounty, I returned to the ..."
          ],
          [
           "The final step in my adventure was to match the le..."
          ],
          [
           "Write a News Article, titled \"SOHO's Comet Legacy\"..."
          ],
          [
           "**SOHO's Comet Legacy: A Decade of Discoveries and..."
          ],
          [
           "In a remarkable achievement, the Solar and Heliosp..."
          ],
          [
           "Since its deployment, SOHO has made a series of gr..."
          ],
          [
           "One of the most notable achievements of SOHO has b..."
          ],
          [
           "The impact of SOHO's discoveries extends far beyon..."
          ],
          [
           "Despite its age, SOHO remains an active and vibran..."
          ],
          [
           "\"We are proud of the incredible contributions that..."
          ],
          [
           "As SOHO approaches the end of its operational life..."
          ],
          [
           "**By the Numbers: SOHO's Comet Legacy**\n\n..."
          ],
          [
           "* Over 4,000 comets detected since 1995\n* Thousand..."
          ],
          [
           "**A Comet Legacy that will be Remembered for Gener..."
          ],
          [
           "The Solar and Heliospheric Observatory (SOHO) spac..."
          ],
          [
           "Write a news feed, titled \"Bolivia's Coca Controve..."
          ],
          [
           "**Bolivia's Coca Controversy**\n\n..."
          ],
          [
           "Bolivia's government is gearing up for a long and ..."
          ],
          [
           "Bolivia's government, under President Evo Morales,..."
          ],
          [
           "The US government has weighed in on the controvers..."
          ],
          [
           "\"Write a Chapter: The COX Code, titled 'Unraveling..."
          ],
          [
           "**Chapter 7: Unraveling the Role of COX Enzymes in..."
          ],
          [
           "The cyclooxygenase (COX) enzymes have been the sub..."
          ],
          [
           "**Introduction**\n\n..."
          ],
          [
           "The COX enzymes are a family of enzymes that play ..."
          ],
          [
           "**Asthma and the COX Enzymes**\n\n..."
          ],
          [
           "Research has shown that COX-2 is overexpressed in ..."
          ],
          [
           "**Cardiovascular Disease and the COX Enzymes**\n\n..."
          ],
          [
           "The role of COX-2 in cardiovascular disease has al..."
          ],
          [
           "**Cancer and the COX Enzymes**\n\n..."
          ],
          [
           "COX-2 is overexpressed in various types of cancer,..."
          ],
          [
           "**Alzheimer's Disease and the COX Enzymes**\n\n..."
          ],
          [
           "Research has also explored the role of COX-2 in Al..."
          ],
          [
           "**Therapeutic Applications of COX Inhibitors**\n\n..."
          ],
          [
           "The potential therapeutic applications of COX inhi..."
          ],
          [
           "**Conclusion**\n\n..."
          ],
          [
           "The COX enzymes play a critical role in various hu..."
          ],
          [
           "**References**\n\n..."
          ],
          [
           "1. Larsson, L., & Barnes, J. (2005). Prostaglandin..."
          ],
          [
           "**Glossary**\n\n..."
          ],
          [
           "* **COX-1**: Cyclooxygenase-1, a constitutively ex..."
          ],
          [
           "Write a News Article titled \"Cosmic Wonder: Rare S..."
          ],
          [
           "**Cosmic Wonder: Rare Sunset Phenomenon Captured o..."
          ],
          [
           "In a breathtaking display of celestial wonder, a r..."
          ],
          [
           "The phenomenon, known as \"sun dogs\" or \"parhelia,\"..."
          ],
          [
           "According to Dr. Maria Rodriguez, a researcher at ..."
          ],
          [
           "The resulting images, which have been shared widel..."
          ],
          [
           "The science behind sun dogs is complex, but essent..."
          ],
          [
           "\"We were lucky to witness this rare phenomenon,\" s..."
          ],
          [
           "The images captured by Dr. Rodriguez and her team ..."
          ],
          [
           "**Related stories:**\n\n..."
          ],
          [
           "* \"Astronomers Witness Rare Solar Eclipse from Rem..."
          ],
          [
           "**Photos:** Dr. Maria Rodriguez's images of the su..."
          ],
          [
           "Write a chapter, titled \"Engaging the Reader: Stra..."
          ],
          [
           "**Engaging the Reader: Strategies for Effective Po..."
          ],
          [
           "As middle school English teachers, we strive to ig..."
          ],
          [
           "One powerful strategy for engaging students in poe..."
          ],
          [
           "Student collaboration is another essential strateg..."
          ],
          [
           "Ultimately, effective poetry analysis requires a s..."
          ],
          [
           "Write a High-Level Overview, titled \"The Downfall ..."
          ],
          [
           "**The Downfall of James II**\n\n..."
          ],
          [
           "King James II of England and Scotland's reign, whi..."
          ],
          [
           "**Early Reign and Rise to Power**\n\n..."
          ],
          [
           "James II, who succeeded his brother Charles II, in..."
          ],
          [
           "**The Test Act and the Association with France**\n\n..."
          ],
          [
           "In 1678, James II signed the Test Act, which furth..."
          ],
          [
           "**The Great Revolution and James II's Disfavor**\n\n..."
          ],
          [
           "In 1688, a group of English nobles, known as the \"..."
          ],
          [
           "**The Glorious Revolution**\n\n..."
          ],
          [
           "The Glorious Revolution, as it came to be known, s..."
          ],
          [
           "**Exile and Later Life**\n\n..."
          ],
          [
           "James II spent the remainder of his life in exile,..."
          ],
          [
           "**Key Themes and Figures**\n\n..."
          ],
          [
           "* **The struggle for power:** James II's reign was..."
          ],
          [
           "**Conclusion**\n\n..."
          ],
          [
           "The downfall of James II was a pivotal moment in E..."
          ],
          [
           "Write a Chapter titled \"Pricing Strategies for Hea..."
          ],
          [
           "**Chapter 7: Pricing Strategies for Healthier Food..."
          ],
          [
           "The way we price food can have a profound impact o..."
          ],
          [
           "One of the most effective pricing strategies for p..."
          ],
          [
           "On the other hand, pricing incentives can also be ..."
          ],
          [
           "The impact of pricing strategies on childhood obes..."
          ],
          [
           "Write a monograph titled \"Sati: A Colonial and Ind..."
          ],
          [
           "**Sati: A Colonial and Indigenous Perspective**\n\n..."
          ],
          [
           "**Introduction**\n\n..."
          ],
          [
           "The practice of sati, or self-immolation of Hindu ..."
          ],
          [
           "**Historical Context**\n\n..."
          ],
          [
           "The practice of sati has its roots in ancient Indi..."
          ],
          [
           "**Colonial Perspective**\n\n..."
          ],
          [
           "From a colonial perspective, sati was seen as a re..."
          ],
          [
           "However, some colonial officials and administrator..."
          ],
          [
           "**Indigenous Perspective**\n\n..."
          ],
          [
           "From an indigenous perspective, sati was a deeply ..."
          ],
          [
           "However, not all Hindu widows participated in the ..."
          ],
          [
           "**Time Periods and Viewpoints**\n\n..."
          ],
          [
           "The debate surrounding sati spanned several decade..."
          ],
          [
           "However, as the British colonial authorities began..."
          ],
          [
           "**Conclusion**\n\n..."
          ],
          [
           "The debate surrounding sati in 19th-century India ..."
          ],
          [
           "This monograph has highlighted the need for a nuan..."
          ],
          [
           "**References**\n\n..."
          ],
          [
           "* Bibesco, O. (1905). _Sati: The Self-Immolation o..."
          ],
          [
           "**Appendices**\n\n..."
          ],
          [
           "* Primary sources:\n\t+ Manusmriti ( translated by G..."
          ],
          [
           "Write a news feed titled \"CDC Autism Prevalence Re..."
          ],
          [
           "**CDC Autism Prevalence Report 2012**\n\n..."
          ],
          [
           "The Centers for Disease Control and Prevention (CD..."
          ],
          [
           "**Prevalence Rates:**\n\n..."
          ],
          [
           "* The prevalence of ASD among 8-year-old children ..."
          ],
          [
           "**Age-Specific Prevalence:**\n\n..."
          ],
          [
           "* The prevalence of ASD was highest among children..."
          ],
          [
           "**Demographic Trends:**\n\n..."
          ],
          [
           "* ASD was more common among non-Hispanic white chi..."
          ],
          [
           "**Co-Occurring Conditions:**\n\n..."
          ],
          [
           "* The report found that 85% of children with ASD a..."
          ],
          [
           "**Geographic Variation:**\n\n..."
          ],
          [
           "* The prevalence of ASD varied across different re..."
          ],
          [
           "**Insights and Implications:**\n\n..."
          ],
          [
           "* The report highlights the need for continued eff..."
          ],
          [
           "Overall, the CDC Autism Prevalence Report 2012 pro..."
          ],
          [
           "Write a Newsletter titled \"Bunchgrasses in Califor..."
          ],
          [
           "**Bunchgrasses in California: The Ultimate Guide t..."
          ],
          [
           "**Volume 1, Issue 1**\n\n..."
          ],
          [
           "As Californians, we are constantly looking for way..."
          ],
          [
           "In this issue of our newsletter, we'll delve into ..."
          ],
          [
           "**Suitable Grasses for California**\n\n..."
          ],
          [
           "California is home to over 30 species of native bu..."
          ],
          [
           "* **Blue grama (Bouteloua gracilis)**: A drought-t..."
          ],
          [
           "**Container Growing and Bunchgrass**\n\n..."
          ],
          [
           "Container growing is an excellent way to introduce..."
          ],
          [
           "* **Choose the right container**: Select a contain..."
          ],
          [
           "**Carbon Sequestration and Bunchgrasses**\n\n..."
          ],
          [
           "Bunchgrasses are a powerful tool for carbon seques..."
          ],
          [
           "* **California's carbon sequestration potential**:..."
          ],
          [
           "**Best Practices for Planting and Maintaining Nati..."
          ],
          [
           "To ensure the success of your native bunchgrass pl..."
          ],
          [
           "* **Choose the right planting time**: In Californi..."
          ],
          [
           "**Opportunities for Carbon Sequestration in Your C..."
          ],
          [
           "If you're interested in exploring opportunities fo..."
          ],
          [
           "* **Native plant restoration projects**: Join or s..."
          ],
          [
           "We hope you found this issue of our newsletter inf..."
          ],
          [
           "Stay tuned for future issues of Bunchgrasses in Ca..."
          ],
          [
           "**Best regards,**\n\n..."
          ],
          [
           "[Your Name]\n\n..."
          ],
          [
           "**Bunchgrasses in California Newsletter Team**\n\n..."
          ],
          [
           "**Stay connected:**\n\n..."
          ],
          [
           "Follow us on social media: @BunchgrassesCA\nSubscri..."
          ],
          [
           "Write a Chapter titled \"Teaching Self-Reliance to ..."
          ],
          [
           "**Chapter 5: Teaching Self-Reliance to Children: A..."
          ],
          [
           "As a parent, there's no greater joy than watching ..."
          ],
          [
           "**Setting Expectations**\n\n..."
          ],
          [
           "Before embarking on the packing journey, it's esse..."
          ],
          [
           "**Creating a Checklist**\n\n..."
          ],
          [
           "To ensure your child is adequately prepared, creat..."
          ],
          [
           "**Providing Constructive Feedback**\n\n..."
          ],
          [
           "Providing constructive feedback is crucial in the ..."
          ],
          [
           "**Gradual Release of Responsibility**\n\n..."
          ],
          [
           "As your child becomes more confident in packing th..."
          ],
          [
           "Write a chapter titled \"The World of Computer Peri..."
          ],
          [
           "**The World of Computer Peripherals**\n\n..."
          ],
          [
           "The world of computer peripherals has undergone si..."
          ],
          [
           "Another critical aspect of computer peripherals is..."
          ],
          [
           "The evolution of computer interface standards has ..."
          ],
          [
           "Write a summary titled \"The Concept of Exactly Loc..."
          ],
          [
           "**The Concept of Exactly Located**\n\n..."
          ],
          [
           "The concept of exactly located refers to a particu..."
          ],
          [
           "At its core, exactly located implies that an objec..."
          ],
          [
           "The concept of exactly located also raises interes..."
          ],
          [
           "Write a Chapter titled \"The Art of Reishiki: Nurtu..."
          ],
          [
           "**Chapter 7: The Art of Reishiki: Nurturing a Cult..."
          ],
          [
           "In the ancient Japanese art of Iaido, the cultivat..."
          ],
          [
           "**The Importance of Etiquette in Iaido Practice**\n..."
          ],
          [
           "Etiquette, or rei, is an integral component of Iai..."
          ],
          [
           "When practicing Iaido, it is essential to remember..."
          ],
          [
           "**The Value of Maintaining a Clean and Respectful ..."
          ],
          [
           "The dojo, or training hall, is a sacred space that..."
          ],
          [
           "Practitioners should strive to maintain a clean an..."
          ],
          [
           "**Embodying Sincere Respect in One's Actions and B..."
          ],
          [
           "Sincere respect is the foundation of Reishiki, and..."
          ],
          [
           "Practitioners should strive to approach their trai..."
          ],
          [
           "**The Importance of Mental Preparation**\n\n..."
          ],
          [
           "Mental preparation is a critical component of Iaid..."
          ],
          [
           "**Conclusion**\n\n..."
          ],
          [
           "The art of Reishiki is a delicate balance of tradi..."
          ],
          [
           "As practitioners, we have a responsibility to upho..."
          ],
          [
           "**Recommendations for Implementing Reishiki in You..."
          ],
          [
           "1. Establish a clear set of dojo etiquette guideli..."
          ],
          [
           "By implementing these recommendations, you can cre..."
          ]
         ],
         "hovertemplate": "type=Real Text<br>Text Length (characters)=%{x}<br>Embedding L2 Norm=%{y}<br>text_truncated=%{customdata[0]}<extra></extra>",
         "legendgroup": "Real Text",
         "marker": {
          "color": "#00cc96",
          "opacity": 0.5,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Real Text",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "AgAFAAwACwAMABAAFgAUABkAHwArACkALwAoAC0AMQBNAUMAWgI5AggC1wH/AXIC/AHMAT0BNADCASQAwQGTAT0AcAA/AiYASQB8AikAIQH5ABUAyACAArYBwgAkAI4CjwKNAgYBKQBOARYAUgArAiAAmgC2ABwAVgAcASUATgCnARgATQDPAagAYwE0AFcCLADbAWMBLACdAbIBJABQACgDLgDJAMkB3wF6ARYAiwJOAkkCOgFeAK4BsQHlAZ4BUAF8ATIBKQBfAUQAiAEgASAAMwL6AWACPwFEAA4CEgCNAiAAtQEwAPQBIABoAS0A7QEwANYBEAB5AhAADQMOAAoEHwE+AFEBigEoASMBCwHWACEBFgDOAIcAhwFDAOsCtQLUAmECbgEeAFgBIwDFATIAzAEyAI4BHQBMARoAoQEcAHUCEABVAU0BPgCIApECtwK7AmIBMQASAFMCGAD6ARoAJAIGAhwA1QF2ASEAxQGRARAAfQFqARAAiwIQAEIBGAEnACMBFwAvAR4AKgEZAKkBHgDcABsAoAAgAJEBAAEHAV8AFwBlAfAAJQC4ANIBJgCvAHwBKwDRADEBRQBgAOMBPgB1ABoC5QCgABMADQAwABUAeAB2AUUALQIaAB8CGgBNAiUAdQInAJMC8AAnAGMCZgLRAsEAJAC/AVQCdwI/AU0A8wEzAGgBEgFGACkBXAE9AN4A+AAqADYBEABFAWkBPAAUAqoA",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "type"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Text Length vs Embedding Norm"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Text Length (characters)"
         },
         "type": "log"
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Embedding L2 Norm"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Collect all data first\n",
    "data = []\n",
    "def add_data(text, text_type):\n",
    "    emb = text2vec.predict([text], source_lang=\"eng_Latn\")\n",
    "    norm = torch.norm(emb).item()\n",
    "    data.append({\n",
    "        'text': text,\n",
    "        'length': len(text),\n",
    "        'norm': norm,\n",
    "        'type': text_type\n",
    "    })\n",
    "\n",
    "# Repeated words (more examples)\n",
    "for length in range(1, 100):\n",
    "    for word in ['word', 'sentence', 'paragraph', 'dog', 'spicy', 'anime']:\n",
    "        words = [word] * length\n",
    "        text = ' '.join(words)\n",
    "        add_data(text, 'Repeated Words')\n",
    "\n",
    "# Random characters (more examples)\n",
    "random.seed(42)\n",
    "for length in range(1, 100, ):\n",
    "        random_words = [''.join(random.choices(string.ascii_lowercase, k=random.randint(3, 8))) for _ in range(length)]\n",
    "        text = ' '.join(random_words)\n",
    "        add_data(text, 'Random Characters')\n",
    "\n",
    "# Normal sentences (many more examples)\n",
    "normal_sentences = [\n",
    "    \"Hi\",\n",
    "    \"Hello\",\n",
    "    \"Good morning\",\n",
    "    \"Hello there\",\n",
    "    \"How are you?\",\n",
    "    \"Nice to meet you\",\n",
    "    \"The cat sat on the mat\",\n",
    "    \"I like to read books\",\n",
    "    \"The weather is nice today\",\n",
    "    \"She went to the store yesterday\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I enjoy listening to music in the evening\",\n",
    "    \"She sells seashells by the seashore on weekends\",\n",
    "    \"To be or not to be, that is the question\",\n",
    "    \"The early bird catches the worm every morning\",\n",
    "    \"A picture is worth a thousand words in most cases\"\n",
    "]\n",
    "for text in normal_sentences:\n",
    "    add_data(text, 'Real Text')\n",
    "\n",
    "# Load dataset of some example texts generated by Llama3b\n",
    "dataset = load_dataset(\"nickypro/fineweb-llama3b-regen-split\", split=\"train\")\n",
    "for split_text in dataset.select(range(20)):\n",
    "    for paragraph in split_text['split_text']:\n",
    "        add_data(paragraph, 'Real Text')\n",
    "\n",
    "\n",
    "# Create DataFrame and plot\n",
    "df = pd.DataFrame(data)\n",
    "# Truncate text to first 50 characters for hover display\n",
    "df['text_truncated'] = df['text'].str[:50] + '...'\n",
    "fig = px.scatter(df,\n",
    "        x='length', y='norm', color='type',\n",
    "        title=\"Text Length vs Embedding Norm\",\n",
    "        labels={'length': 'Text Length (characters)', 'norm': 'Embedding L2 Norm'},\n",
    "        hover_data=['text_truncated'],\n",
    "        opacity=0.5,\n",
    "        log_x=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf1d3b-c2ca-4e5d-b326-3ef3404126ac",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 4: Token Swapping Experiments\n",
    "\n",
    " This exercise explores how we can manipulate text embeddings to perform token swapping.\n",
    " We'll investigate:\n",
    " 1. Building difference vectors between similar texts\n",
    " 2. Applying global transformations to swap words\n",
    " 3. Creating position-specific transformations for targeted edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70efdce-58a0-42ea-a0fa-ee0a6809d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def diff_vector(src_text: str, tgt_text: str) -> Float[torch.Tensor, \"1024\"]:\n",
    "    \"\"\"Return embedding difference between *tgt_text* and *src_text* (tgt − src).\"\"\"\n",
    "    # [your implementation here]\n",
    "    src_emb = text2vec.predict([src_text], source_lang=\"eng_Latn\")\n",
    "    tgt_emb = text2vec.predict([tgt_text], source_lang=\"eng_Latn\")\n",
    "    return (tgt_emb - src_emb).squeeze(0)\n",
    "\n",
    "def decode(embedding: torch.Tensor, max_seq_len: int = 512) -> str:\n",
    "    \"\"\"Greedy‑decode a single 1024‑D embedding back to text.\"\"\"\n",
    "    # [your implementation here]\n",
    "    return vec2text.predict(embedding.unsqueeze(0), target_lang=\"eng_Latn\", max_seq_len=max_seq_len)[0]\n",
    "\n",
    "\n",
    "def positional_diff(src_word: str, tgt_word: str, pos: int, *, seq_len: int, filler: str = \"_\") -> torch.Tensor:\n",
    "    \"\"\"Build a difference vector that swaps **src_word→tgt_word** at index *pos*.\n",
    "\n",
    "    All other positions are filled with *filler* tokens so that the vector is\n",
    "    specific to that location.\n",
    "    \"\"\"\n",
    "    # [your implementation here]\n",
    "    src_tokens = [filler] * seq_len\n",
    "    tgt_tokens = [filler] * seq_len\n",
    "    src_tokens[pos] = src_word\n",
    "    tgt_tokens[pos] = tgt_word\n",
    "    return diff_vector(\" \".join(src_tokens), \" \".join(tgt_tokens))\n",
    "\n",
    "assert diff_vector(\"dog\", \"cat\").shape == (1024,)\n",
    "assert isinstance(decode(torch.randn(1024), 5), str)\n",
    "assert positional_diff(\"dog\", \"cat\", pos=1, seq_len=8, filler=\"a\").shape == (1024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53868e7-10cd-4bb6-85e9-d6d0043fca49",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Now we can try see what the difference vector does in different cases.\n",
    " 1. Global dog→cat vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f794ca2-8f0b-4128-94f7-b8fdf42011e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Global word swapping:\n",
      "Original:               the dog is happy in the dog house\n",
      "Global swap dog→cat:    The cat is happy in the cat house\n",
      "\n",
      "2. Position-specific swapping:\n",
      "Position‑aware swap:    the cat is happy in the dog house\n",
      "\n",
      "3. Testing different word pairs:\n",
      "happy→sad: 'the happy animal lives here' → 'the sad animal lives here'\n",
      "house→tree: 'the house animal lives here' → 'the tree animal lives here'\n",
      "big→small: 'the big animal lives here' → 'the small animal lives here'\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Global word swapping:\")\n",
    "swap_vec = diff_vector(\"dog\", \"cat\")\n",
    "sentence = \"the dog is happy in the dog house\"\n",
    "sent_emb = text2vec.predict([sentence], source_lang=\"eng_Latn\").squeeze(0)\n",
    "\n",
    "print(f\"Original:               {decode(sent_emb)}\")\n",
    "print(f\"Global swap dog→cat:    {decode(sent_emb + swap_vec)}\")\n",
    "\n",
    "# 2. Position‑specific swap\n",
    "print(\"\\n2. Position-specific swapping:\")\n",
    "# Swap only the token at index 1 (0‑based) in a sentence\n",
    "pos_vec = positional_diff(\"dog\", \"cat\", pos=1, seq_len=8, filler=\"a\")\n",
    "print(f\"Position‑aware swap:    {decode(sent_emb + pos_vec)}\")\n",
    "\n",
    "# 3. Test with different word pairs\n",
    "print(\"\\n3. Testing different word pairs:\")\n",
    "word_pairs = [(\"happy\", \"sad\"), (\"house\", \"tree\"), (\"big\", \"small\")]\n",
    "for src, tgt in word_pairs:\n",
    "    swap_vec = diff_vector(src, tgt)\n",
    "    test_sentence = f\"the {src} animal lives here\"\n",
    "    test_emb = text2vec.predict([test_sentence], source_lang=\"eng_Latn\").squeeze(0)\n",
    "    print(f\"{src}→{tgt}: '{test_sentence}' → '{decode(test_emb + swap_vec)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b4490-2107-4e20-a004-d6acf0adc269",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 5: Sentence Combination\n",
    "\n",
    " This exercise explores how we can combine two sentences into a single embedding.\n",
    " So far I have only tried a couple of the most naive approaches. It's ok but I suspect it should be easy to try better approaches to this also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a423f2-4e22-4a8f-9c81-a1255eccf961",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 1: Basic Combination Analysis\n",
    "\n",
    " First, let's analyze how SONAR combines sentences with different relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae13eaf-7aaf-4f48-9255-4cf476ed37e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Combination Analysis:\n",
      "                           sent_a                            sent_b  sim_ab_a  sim_ab_b  sim_ab_ba  sim_ab_avg  sim_ab_sum  order_sensitivity\n",
      "   The weather is beautiful today        I think I'll go for a walk  0.667883  0.626139   0.536822    0.790461    0.790461           0.211020\n",
      "She opened the mysterious lett... Her hands trembled as she read...  0.606783  0.603015   0.585894    0.721195    0.721195           0.210878\n",
      "                I love sunny days               But I hate the rain  0.764754  0.719204   0.696757    0.824110    0.824110           0.165407\n",
      "           The movie was exciting However, the ending disappoint...  0.593056  0.747074   0.665652    0.793042    0.793042           0.167145\n",
      "     Cats are independent animals Python is a programming langua...  0.554871  0.687045   0.659140    0.751446    0.751446           0.189619\n",
      "         The Earth orbits the Sun         Pizza is my favorite food  0.485667  0.600160   0.513822    0.706703    0.706703           0.232466\n",
      "      What's your favorite color?         My favorite color is blue  0.749823  0.824498   0.712083    0.864491    0.864491           0.165408\n",
      "               Where do you live?           I live in New York City  0.662529  0.804851   0.711155    0.827463    0.827463           0.141202\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Create diverse sentence pairs for analysis\n",
    "sentence_pairs = [\n",
    "    # Related sentences (continuation)\n",
    "    (\"The weather is beautiful today\", \"I think I'll go for a walk\"),\n",
    "    (\"She opened the mysterious letter\", \"Her hands trembled as she read it\"),\n",
    "\n",
    "    # Contrasting sentences\n",
    "    (\"I love sunny days\", \"But I hate the rain\"),\n",
    "    (\"The movie was exciting\", \"However, the ending disappointed me\"),\n",
    "\n",
    "    # Unrelated sentences\n",
    "    (\"Cats are independent animals\", \"Python is a programming language\"),\n",
    "    (\"The Earth orbits the Sun\", \"Pizza is my favorite food\"),\n",
    "\n",
    "    # Question-answer pairs\n",
    "    (\"What's your favorite color?\", \"My favorite color is blue\"),\n",
    "    (\"Where do you live?\", \"I live in New York City\"),\n",
    "]\n",
    "\n",
    "# Analyze combinations\n",
    "combination_data = []\n",
    "for sent_a, sent_b in sentence_pairs:\n",
    "    # Individual embeddings\n",
    "    emb_a = text2vec.predict([sent_a], source_lang=\"eng_Latn\")\n",
    "    emb_b = text2vec.predict([sent_b], source_lang=\"eng_Latn\")\n",
    "\n",
    "    # Combined embeddings (both orders)\n",
    "    combined_ab = f\"{sent_a} {sent_b}\"\n",
    "    combined_ba = f\"{sent_b} {sent_a}\"\n",
    "    emb_ab = text2vec.predict([combined_ab], source_lang=\"eng_Latn\")\n",
    "    emb_ba = text2vec.predict([combined_ba], source_lang=\"eng_Latn\")\n",
    "\n",
    "    # Various combinations\n",
    "    emb_avg = (emb_a + emb_b) / 2\n",
    "    emb_sum = emb_a + emb_b\n",
    "    emb_diff = emb_a - emb_b\n",
    "\n",
    "    # Calculate similarities\n",
    "    data = {\n",
    "        'sent_a': sent_a[:30] + '...' if len(sent_a) > 30 else sent_a,\n",
    "        'sent_b': sent_b[:30] + '...' if len(sent_b) > 30 else sent_b,\n",
    "        'sim_ab_a': torch.nn.functional.cosine_similarity(emb_ab, emb_a).item(),\n",
    "        'sim_ab_b': torch.nn.functional.cosine_similarity(emb_ab, emb_b).item(),\n",
    "        'sim_ab_ba': torch.nn.functional.cosine_similarity(emb_ab, emb_ba).item(),\n",
    "        'sim_ab_avg': torch.nn.functional.cosine_similarity(emb_ab, emb_avg).item(),\n",
    "        'sim_ab_sum': torch.nn.functional.cosine_similarity(emb_ab, emb_sum).item(),\n",
    "        'order_sensitivity': torch.norm(emb_ab - emb_ba).item()\n",
    "    }\n",
    "    combination_data.append(data)\n",
    "\n",
    "# Display results\n",
    "df_comb = pd.DataFrame(combination_data)\n",
    "print(\"Sentence Combination Analysis:\")\n",
    "print(df_comb.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d0aaa-854a-47a4-9e0d-5f0405f38bc8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 2: Try simple linear combination\n",
    " If we want to combine two sentences, we can just add their embeddings? Or maybe average them? Will this give us something that works as an embedding with two sentences side-by-side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00701b-da0f-4470-a805-836e6f110efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sent1: It started raining heavily.\n",
      "Sent2: Everyone ran for shelter.\n",
      "True: It started to rain heavily and everyone ran for shelter.\n",
      "Pred: He started to run.\n",
      "Similarity: 0.7860\n",
      "\n",
      "Sent1: First, preheat the oven.\n",
      "Sent2: Then, mix the ingredients.\n",
      "True: First, preheat the oven. Then, mix the ingredients.\n",
      "Pred: Then, preheat.\n",
      "Similarity: 0.7897\n",
      "\n",
      "Sent1: The book was fascinating.\n",
      "Sent2: The movie adaptation was terrible.\n",
      "True: The book was fascinating. The film adaptation was terrible.\n",
      "Pred: The movie was fascinating.\n",
      "Similarity: 0.7625\n",
      "\n",
      "Sent1: I need to buy milk.\n",
      "Sent2: I also need to get bread.\n",
      "True: I need to buy milk. I also need to get bread.\n",
      "Pred: I need to get some milk.\n",
      "Similarity: 0.8186\n",
      "\n",
      "Average similarity: 0.7892\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class SimpleLinearCombiner(nn.Module):\n",
    "    def __init__(self, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "basic_combiner_model = SimpleLinearCombiner().to(DEVICE)\n",
    "\n",
    "# Test the simple linear combiner\n",
    "def test_performance_on_new_examples(model, verbose=True):\n",
    "    \"\"\"Test model performance on predefined pairs plus one random example\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Predefined test pairs\n",
    "    test_pairs = [\n",
    "        (\"It started raining heavily.\", \"Everyone ran for shelter.\"),\n",
    "        (\"First, preheat the oven.\", \"Then, mix the ingredients.\"),\n",
    "        (\"The book was fascinating.\", \"The movie adaptation was terrible.\"),\n",
    "        (\"I need to buy milk.\", \"I also need to get bread.\"),\n",
    "    ]\n",
    "\n",
    "    # Add one random pair\n",
    "    # idx1, idx2 = np.random.choice(len(all_sentences), 2, replace=False)\n",
    "    # test_pairs.append((all_sentences[idx1], all_sentences[idx2]))\n",
    "\n",
    "    test_results = []\n",
    "\n",
    "    for sent1, sent2 in test_pairs:\n",
    "        # Get embeddings\n",
    "        emb1 = text2vec.predict([sent1], source_lang=\"eng_Latn\").to(DEVICE)\n",
    "        emb2 = text2vec.predict([sent2], source_lang=\"eng_Latn\").to(DEVICE)\n",
    "        emb_true = text2vec.predict([f\"{sent1} {sent2}\"], source_lang=\"eng_Latn\").to(DEVICE)\n",
    "\n",
    "        # Predict and decode\n",
    "        with torch.no_grad():\n",
    "            emb_pred = model(emb1.squeeze(0), emb2.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        text_true = vec2text.predict(emb_true.cpu(), target_lang=\"eng_Latn\")[0]\n",
    "        text_pred = vec2text.predict(emb_pred.cpu(), target_lang=\"eng_Latn\")[0]\n",
    "        similarity = torch.cosine_similarity(emb_pred, emb_true, dim=-1).item()\n",
    "\n",
    "        test_results.append({\n",
    "            'sent1': sent1, 'sent2': sent2, 'decoded_true': text_true,\n",
    "            'decoded_pred': text_pred, 'similarity': similarity\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nSent1: {sent1}\")\n",
    "            print(f\"Sent2: {sent2}\")\n",
    "            print(f\"True: {text_true}\")\n",
    "            print(f\"Pred: {text_pred}\")\n",
    "            print(f\"Similarity: {similarity:.4f}\")\n",
    "\n",
    "    avg_similarity = np.mean([r['similarity'] for r in test_results])\n",
    "    print(f\"\\nAverage similarity: {avg_similarity:.4f}\")\n",
    "    return test_results\n",
    "\n",
    "# Test the simple linear combiner\n",
    "test_results = test_performance_on_new_examples(basic_combiner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6b5e3-9275-4d42-8d1b-cd7cbcc9c0f5",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 3: Better ways of combining sentences.\n",
    "\n",
    " We can try to do better than just a simple linear combination to try get behaviour like concatenation. For this, we will need some training data.\n",
    "\n",
    " We'll create a dataset of sentence pairs and their combined embeddings to train our model.\n",
    " As a source of data, use the provided dataset of llama-3.2-3b-instruct generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dcaa9-558b-4f7d-bdf5-33f9074e3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting training data...\n",
      "Collected 2000 sentences\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "print(\"Getting training data...\")\n",
    "dataset = load_dataset(\"nickypro/fineweb-llama3b-regen-split\", split=\"train\")\n",
    "# Extract individual sentences\n",
    "all_sentences = []\n",
    "for item in dataset.select(range(100)):  # Use first 100 documents\n",
    "    for paragraph in item['split_text']:\n",
    "        # Split paragraph into sentences (simple approach)\n",
    "        sentences = paragraph.split('. ')\n",
    "        for sent in sentences:\n",
    "            if 10 < len(sent) < 200:  # Filter by length\n",
    "                all_sentences.append(sent.strip())\n",
    "# Limit to manageable size\n",
    "all_sentences = all_sentences[:2000]\n",
    "print(f\"Collected {len(all_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4a6eb-4988-403e-9a87-4573c628e564",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " What do you see?\n",
    " In general, you should see that this kinda gets a sentence that is the same as one of the original sentences, or inbetween the two sentences. It doesn't really append one sentence to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cf906-5311-4db1-b06a-195589e960e7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 4: Create Training Data for Sentence Combination\n",
    "\n",
    " Now we need to create training data to teach our model how to combine sentence embeddings.\n",
    " The goal is to learn a function that maps two individual sentence embeddings to the embedding\n",
    " of their concatenation.\n",
    "\n",
    " **Your task**: Create pairs of sentences and compute their embeddings along with the embedding\n",
    " of their concatenated form. This will give us input-output pairs for training.\n",
    "\n",
    " **Steps to implement**:\n",
    " 1. Randomly select pairs of sentences from our collected sentences\n",
    " 2. Compute embeddings for each individual sentence using SONAR\n",
    " 3. Create a concatenated sentence by joining them with a space\n",
    " 4. Compute the embedding of the concatenated sentence (this is our target)\n",
    " 5. Store all embeddings and original text for training\n",
    "\n",
    " **Expected outcome**: A dataset where each example contains:\n",
    " - Original sentences text for reference\n",
    " - Two individual sentence embeddings (inputs)\n",
    " - The embedding of their concatenation (target output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66b5f4-cfeb-4824-a70f-018ba89eb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sentence pairs and embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 38.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 training examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def create_training_data(all_sentences: list[str], n_pairs: int = 1000) -> list[dict]:\n",
    "    \"\"\"Create training data for sentence combination.\n",
    "\n",
    "    Args:\n",
    "        all_sentences: List of sentences to create training data from.\n",
    "        n_pairs: Number of pairs to create.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with training data.\n",
    "        Each dictionary contains:\n",
    "        - 'sent1': First sentence\n",
    "        - 'sent2': Second sentence\n",
    "        - 'emb1': Embedding of the first sentence\n",
    "        - 'emb2': Embedding of the second sentence\n",
    "        - 'emb_combined': Embedding of the concatenated sentence\n",
    "    \"\"\"\n",
    "    print(\"Creating sentence pairs and embeddings...\")\n",
    "    training_data = []\n",
    "\n",
    "    for i in tqdm(range(n_pairs)):\n",
    "        # Randomly select two sentences\n",
    "        # [your implementation here]\n",
    "        idx1, idx2 = np.random.choice(len(all_sentences), 2, replace=False)\n",
    "        sent1, sent2 = all_sentences[idx1], all_sentences[idx2]\n",
    "\n",
    "        # Compute embeddings\n",
    "        emb1 = text2vec.predict([sent1], source_lang=\"eng_Latn\")\n",
    "        emb2 = text2vec.predict([sent2], source_lang=\"eng_Latn\")\n",
    "\n",
    "        # Compute combined embedding\n",
    "        combined = f\"{sent1} {sent2}\"\n",
    "        emb_combined = text2vec.predict([combined], source_lang=\"eng_Latn\")\n",
    "        # [~# end of exercise]\n",
    "\n",
    "        training_data.append({\n",
    "            'sent1': sent1,\n",
    "            'sent2': sent2,\n",
    "            'emb1': emb1.cpu(),\n",
    "            'emb2': emb2.cpu(),\n",
    "            'emb_combined': emb_combined.cpu(),\n",
    "        })\n",
    "\n",
    "    print(f\"Generated {len(training_data)} training examples\")\n",
    "    return training_data\n",
    "\n",
    "training_data = create_training_data(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31a309-19a2-427b-8cd2-7605484a096d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 5: Trained scale combination model\n",
    "\n",
    " Now let's create a more sophisticated model that learns how to combine two sentence embeddings.\n",
    " This model will have learnable parameters that can be optimized to better concatenate sentences.\n",
    "\n",
    " **Exercise**: Implement a ScaleCombinerModel that learns optimal weights for combining embeddings:\n",
    " - Initialize learnable scale parameters for each input embedding\n",
    " - Add a learnable constant bias term\n",
    " - The output should be: const + scale1*embedding1 + scale2*embedding2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e36fb2-ed11-4792-8888-1025c8e91fee",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " define the simple scaled linear combiner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ff952-afb4-4faf-8d22-0bb41b9bcf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,026\n"
     ]
    }
   ],
   "source": [
    "class ScaleCombinerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear combiner model:\n",
    "    output = const + (scale1)*x + (scale2)*y\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Constant bias\n",
    "        self.const = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "        # Scalar weights for original embeddings\n",
    "        self.scale1 = nn.Parameter(torch.ones(1) * 0.5)\n",
    "        self.scale2 = nn.Parameter(torch.ones(1) * 0.5)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Simple linear combination\n",
    "        # [your implementation here]\n",
    "        output = self.const + self.scale1 * x + self.scale2 * y\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scale_combiner_model = ScaleCombinerModel(embed_dim=1024).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in scale_combiner_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b2053-da7d-4261-91f0-70111e5f56b3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Write the training loop for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69acea-c54c-4012-802f-fe9cbd20e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a combiner model...\n",
      "Epoch 1: Train Loss = 0.0000, Test Loss = 0.0000\n",
      "Epoch 21: Train Loss = 0.0000, Test Loss = 0.0000\n",
      "Epoch 41: Train Loss = 0.0000, Test Loss = 0.0000\n",
      "Epoch 61: Train Loss = 0.0000, Test Loss = 0.0000\n",
      "Epoch 81: Train Loss = 0.0000, Test Loss = 0.0000\n",
      "Epoch 100: Train Loss = 0.0000, Test Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class CombinerModelTrainer:\n",
    "    \"\"\"Trainer class for the ScaleCombinerModel.\"\"\"\n",
    "\n",
    "    def __init__(self, model, device=None):\n",
    "        self.model = model\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def prepare_data(self, training_data, test_size=0.2, random_state=42):\n",
    "        \"\"\"Prepare training data by stacking embeddings and splitting train/test.\"\"\"\n",
    "        X1 = torch.stack([d['emb1'].squeeze(0) for d in training_data])\n",
    "        X2 = torch.stack([d['emb2'].squeeze(0) for d in training_data])\n",
    "        Y = torch.stack([d['emb_combined'].squeeze(0) for d in training_data])\n",
    "\n",
    "        # Split into train/test\n",
    "        X1_train, X1_test, X2_train, X2_test, Y_train, Y_test = train_test_split(\n",
    "            X1, X2, Y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        self.X1_train = X1_train.to(self.device)\n",
    "        self.X2_train = X2_train.to(self.device)\n",
    "        self.Y_train = Y_train.to(self.device)\n",
    "        self.X1_test = X1_test.to(self.device)\n",
    "        self.X2_test = X2_test.to(self.device)\n",
    "        self.Y_test = Y_test.to(self.device)\n",
    "\n",
    "    def train_epoch(self, optimizer, criterion, batch_size=32):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i in range(0, len(self.X1_train), batch_size):\n",
    "            # [your implementation here]\n",
    "            batch_x1 = self.X1_train[i:i+batch_size]\n",
    "            batch_x2 = self.X2_train[i:i+batch_size]\n",
    "            batch_y = self.Y_train[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.model(batch_x1, batch_x2)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def evaluate(self, criterion):\n",
    "        \"\"\"Evaluate model on train and test sets.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # [your implementation here]\n",
    "            train_pred = self.model(self.X1_train, self.X2_train)\n",
    "            train_loss = criterion(train_pred, self.Y_train).item()\n",
    "\n",
    "            test_pred = self.model(self.X1_test, self.X2_test)\n",
    "            test_loss = criterion(test_pred, self.Y_test).item()\n",
    "\n",
    "        return train_loss, test_loss\n",
    "\n",
    "    def train(self, training_data, epochs=100, lr=1e-3, batch_size=32, verbose=True):\n",
    "        \"\"\"Train the combiner model on the provided training data.\"\"\"\n",
    "        # Prepare data\n",
    "        self.prepare_data(training_data)\n",
    "\n",
    "        # Training setup\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Reset loss tracking\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training a combiner model...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            epoch_loss = self.train_epoch(optimizer, criterion, batch_size)\n",
    "\n",
    "            # Evaluation\n",
    "            train_loss, test_loss = self.evaluate(criterion)\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.test_losses.append(test_loss)\n",
    "\n",
    "            if verbose and (epoch % 20 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "        return self.train_losses, self.test_losses\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    torch.set_grad_enabled(True)  # We're now training but only in this cell\n",
    "    trainer = CombinerModelTrainer(scale_combiner_model, DEVICE)\n",
    "    train_losses, test_losses = trainer.train(training_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error training model: {e}\")\n",
    "    if hasattr(e, 'traceback'):\n",
    "        print(e.traceback)\n",
    "finally:\n",
    "    torch.set_grad_enabled(False)  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9716d7-324b-47ab-93b4-43927909ec6f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 6: Test Performance on New Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9265bd-0887-4bd7-87cc-3c07ce6f5816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Test Examples:\n",
      "================================================================================\n",
      "\n",
      "Sent1: It started raining heavily.\n",
      "Sent2: Everyone ran for shelter.\n",
      "True: It started to rain heavily and everyone ran for shelter.\n",
      "Pred: Everyone started raining hard to run rough. It began to rain.\n",
      "Similarity: 0.7841\n",
      "\n",
      "Sent1: First, preheat the oven.\n",
      "Sent2: Then, mix the ingredients.\n",
      "True: First, preheat the oven. Then, mix the ingredients.\n",
      "Pred: Then, first, preheat the ingredients. Warm the oven.\n",
      "Similarity: 0.7896\n",
      "\n",
      "Sent1: The book was fascinating.\n",
      "Sent2: The movie adaptation was terrible.\n",
      "True: The book was fascinating. The film adaptation was terrible.\n",
      "Pred: The book adaptation was amazing. The film was fascinating.\n",
      "Similarity: 0.7626\n",
      "\n",
      "Sent1: I need to buy milk.\n",
      "Sent2: I also need to get bread.\n",
      "True: I need to buy milk. I also need to get bread.\n",
      "Pred: I need to get some milk I need to buy bread. I need some milk.\n",
      "Similarity: 0.8099\n",
      "\n",
      "Average similarity: 0.7865\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "print(\"\\nModel Performance on Test Examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = test_performance_on_new_examples(scale_combiner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c6110-61dd-4e8d-8f81-3c90501aa47a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " What do you see?\n",
    " It does a better job, it seems to be approximately one sentence followed by the other, but kind still mixes the two sentences up a but sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1370cc-813e-4dca-a39b-555745390704",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Bonus Exercise: Try to improve the model.\n",
    " Maybe there are better ways to combine the sentences to get concat? Can you get it so that it reliably concatenates two sentences in the correct order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ab189-eb9b-41ec-94b7-89890fc07c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m     21\u001b[39m DEVICE = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m better_combiner_model = \u001b[43mBetterCombinerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m.to(DEVICE)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mbetter_combiner_model.parameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mBetterCombinerModel.__init__\u001b[39m\u001b[34m(self, embed_dim)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.const = nn.Parameter(torch.zeros(embed_dim))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# other parameters\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# [your code here]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class BetterCombinerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear combiner model:\n",
    "    output = const + (scale1)*x + (scale2)*y\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        # Constant bias\n",
    "        self.const = nn.Parameter(torch.zeros(embed_dim))\n",
    "        # other parameters\n",
    "        # [your code here]\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # [your code here]\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Initialize model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "better_combiner_model = BetterCombinerModel(embed_dim=1024).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in better_combiner_model.parameters()):,}\")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    torch.set_grad_enabled(True)  # We're now training but only in this cell\n",
    "    trainer = CombinerModelTrainer(better_combiner_model, DEVICE)\n",
    "    train_losses, test_losses = trainer.train(training_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error training model: {e}\")\n",
    "    if hasattr(e, 'traceback'):\n",
    "        print(e.traceback)\n",
    "finally:\n",
    "    torch.set_grad_enabled(False)  #"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
