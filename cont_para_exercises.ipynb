{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2c5550-389e-4921-88ea-0d49ae212d5a",
   "metadata": {},
   "source": [
    " ## Continuation ParaScope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea2400-9c31-4571-8621-a9f59a0b4772",
   "metadata": {},
   "source": [
    " Install necessary packages for transformer models and evaluation metrics.\n",
    " Just run and continue unless there are errors.\n",
    " you may need to install some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d2722-376d-48ec-8cb6-01fafd3141af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# !pip install -q transformer-lens plotly pandas matplotlib seaborn numpy scikit-learn datasets transformers sentence-transformers\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from typing import Callable\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90952b4-1d18-4a40-9124-19f49208b07a",
   "metadata": {},
   "source": [
    " # Continuation ParaScope Implementation\n",
    "\n",
    " ParaScope (Paragraph Scope) is a method for extracting paragraph-level information from language model residual streams.\n",
    " The core idea is to decode what a language model is \"planning\" to write in an upcoming paragraph by analyzing the\n",
    " residual stream activations at transition points (like \"\\n\\n\" tokens).\n",
    "\n",
    " ![ParaScope Illustration](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7421b220f111e4736b9a8d5a7bae0d0267d1b321fbda5461.png)\n",
    "\n",
    " **Continuation ParaScope** is the simplest approach:\n",
    " 1. Extract residual stream activations at a \"\\n\\n\" token from some original text\n",
    " 2. Create a minimal prompt with just \"<bos>\\n\\n\"\n",
    " 3. Replace the residual activations of the \"\\n\\n\" token with the saved activations\n",
    " 4. Generate text to see what the model \"planned\" to write\n",
    "\n",
    " This tests whether language models encode information about upcoming paragraphs in their residual streams,\n",
    " providing evidence for either explicit or implicit planning in language generation.\n",
    "\n",
    "\n",
    " ![Continuation Parascope Illustration](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bb0725d6fb774228484beadd06dd8af5185f7c7d539b7090.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c518da6-63f9-4ebd-80e8-9d07ec5318d3",
   "metadata": {},
   "source": [
    " ## Setup and Installation\n",
    "\n",
    " We start by loading the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90063d62-3552-44fe-acae-6661c5caa54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# print(\"Available hook points:\")\n",
    "# for name in list(model.hook_dict.keys())[:28]:  # Show first few hook points\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71dcdb-f863-42d6-b102-d8c86e474b5f",
   "metadata": {},
   "source": [
    " ### Prompting and relevant scaffolding\n",
    " Some functions to help with prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daef75-2f09-46c1-acbc-a5204576f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def format_prompt(prompt: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create tokenized chat using the model's tokenizer with chat template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template and tokenize\n",
    "    return model.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "def two_topics_prompt(topic1: str, topic2: str) -> torch.Tensor:\n",
    "    prompt = f\"\"\"\n",
    "Please write two paragraphs, each of length 2 sentences. The first should describe {topic1}, and the second should describe {topic2}. Do not say explicitly name the topic, and do not say anything else.\n",
    "\"\"\"\n",
    "    return format_prompt(prompt)\n",
    "\n",
    "def extract_second_paragraph(text: str) -> str:\n",
    "    return text.split(\"\\n\\n\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4d1ef-eeb2-4fd2-aaee-9572073fdcde",
   "metadata": {},
   "source": [
    " ### Baseline Generation\n",
    " Try running the model with just the prompt to get a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6595777-201a-4d53-8af6-3f80c5d61884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "prompt = two_topics_prompt(\"monster trucks\", \"paracetamol\")\n",
    "prompt_tokens = model.to_tokens(prompt)[:, 1:] # remove double <bos>\n",
    "\n",
    "# Generate context based on the string\n",
    "baseline_generated_tokens = model.generate(prompt_tokens, max_new_tokens=200, do_sample=True, temperature=0.3)\n",
    "output_tokens = baseline_generated_tokens[:, len(prompt_tokens[0]):]\n",
    "output_str_tokens = model.to_str_tokens(output_tokens)\n",
    "print(model.to_str_tokens(prompt_tokens))\n",
    "print(output_str_tokens)\n",
    "\n",
    "# get first paragraph\n",
    "# Find the index of the first occurrence of '\\n\\n' in the output tokens\n",
    "index_of_first_newline = None\n",
    "for i, token in enumerate(output_str_tokens):\n",
    "    if '\\n\\n' in token:\n",
    "        index_of_first_newline = i\n",
    "        break\n",
    "\n",
    "assert index_of_first_newline is not None\n",
    "print(f\"Index of first newline: {index_of_first_newline} ({[output_str_tokens[index_of_first_newline]]})\")\n",
    "\n",
    "par1_tokens = output_tokens[:, :index_of_first_newline+1]\n",
    "par2_tokens = output_tokens[:, index_of_first_newline+1:]\n",
    "par1_str = model.to_string(par1_tokens)[0]\n",
    "par2_str = model.to_string(par2_tokens)[0]\n",
    "prompt_with_par1 = torch.cat((prompt_tokens, par1_tokens), dim=-1)\n",
    "print(par1_str)\n",
    "print(par2_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd945e01-a414-4b27-b4ab-2fb6aec8b72e",
   "metadata": {},
   "source": [
    " Define the hooks that we'll use to capture the activations\n",
    "\n",
    " We'll use the `store_hook` method to store the activations, and the `modify_hook` method to modify the activations.\n",
    "\n",
    " The `modify_hook` method is given the activation and the hook, and should return the modified activation only if the hook has not been seen before. (this prevents the scenario where you keep getting the same output over and over)\n",
    " We can optionally scale the changed activations by multiplying by a scalar modify_scaling_factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240508b-b813-4078-80b1-207462adf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActStore:\n",
    "    def __init__(self, model: HookedTransformer):\n",
    "        self.act_store = {}\n",
    "        self.act_seen = set()\n",
    "\n",
    "        chosen_hooks = ['hook_resid_pre', 'hook_resid_mid']\n",
    "        Ls = range(model.cfg.n_layers) # all layers, you can test with fewer layers\n",
    "        self.chosen_hook_list = \\\n",
    "            [f'blocks.{i}.{h}' for h in chosen_hooks for i in Ls]\n",
    "        self.modify_scaling_factor = 1.0\n",
    "        self.modify_hook_token_idx = -1\n",
    "\n",
    "    def store_hook(self, act, hook):\n",
    "        \"\"\"\n",
    "        Store the activations for the given hook, based on the hook name.\n",
    "        \"\"\"\n",
    "        # [your implementation here]\n",
    "        raise NotImplementedError(\"You need to implement this.\")\n",
    "\n",
    "    def modify_hook(self, act, hook):\n",
    "        \"\"\"\n",
    "        Modify the activations for the given hook at modify_hook_token_idx, based on the hook name. Optionally scale the source activations by modify_scaling_factor.\n",
    "        \"\"\"\n",
    "        # should we modify the act, or have we already done so?\n",
    "        if hook.name in self.act_seen:\n",
    "            return act\n",
    "        self.act_seen.add(hook.name)\n",
    "\n",
    "        # modify the act\n",
    "        # [your implementation here]\n",
    "        raise NotImplementedError(\"You need to implement this.\")\n",
    "\n",
    "    def modify_hook_list(self):\n",
    "        \"return a list of hooks to modify, and reset the act_seen set to empty\"\n",
    "        self.act_seen = set()\n",
    "        return [(hook, self.modify_hook) for hook in self.chosen_hook_list]\n",
    "\n",
    "    def store_hook_list(self):\n",
    "        \"return a list of hooks to store\"\n",
    "        return [(hook, self.store_hook) for hook in self.chosen_hook_list]\n",
    "\n",
    "act_store = ActStore(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72881a8-42ce-49c3-ba0c-5b199ecbf547",
   "metadata": {},
   "source": [
    " ## Generate with transferred activations.\n",
    " This technique demonstrates \"activation patching\" or \"activation transfer\" - a method for transferring learned context from one prompt to another by manipulating the model's internal representations.\n",
    "\n",
    " The process works as follows:\n",
    " 1. Run the model on a \"source\" prompt that contains rich context (e.g., a paragraph with specific style/content)\n",
    " 2. Store the intermediate activations (residual stream values) from all the layers for the token of interest on the source prompt\n",
    " 3. Run the model on a \"target\" prompt that lacks context (e.g., just \"\\n\\n\")\n",
    " 4. During the target generation, replace the activations at the same positions with the stored activations from the source\n",
    " 5. This allows the model to generate text as if it had the original context, even though the target prompt is minimal\n",
    "\n",
    " This technique reveals how much contextual information is encoded in the model's intermediate representations\n",
    " and can be used to study how different types of context (style, topic, format) are stored and utilized.\n",
    " Get the full string (example + generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e75024-c224-4c26-aff4-c6da9cc5fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_activations(tokens: torch.Tensor, num_copies: int = 10, num_tokens: int = 30):\n",
    "    \"\"\"\n",
    "    Function that:\n",
    "    - runs the model once with the original prompt, with hooks to save the activations\n",
    "    - Creates a new prompt with \"\\n\\n\" repeated num_copies times\n",
    "    - then runs the model again with the new prompt, with hooks modifying the activations at the final token to match the original, ang generates many tokens.\n",
    "    - returns List[torch.Tensor] of generated tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # [your implementation here]\n",
    "    raise NotImplementedError(\"You need to implement this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab768167-deda-477b-ba52-c3784ef31e3e",
   "metadata": {},
   "source": [
    " ## Demonstrating Activation Transfer\n",
    "\n",
    " Now we'll test our activation transfer function on different prompts to see how well it preserves\n",
    " and transfers contextual information. We'll run the function on:\n",
    " 1. The original single paragraph prompt to see how it continues the established style/content\n",
    " 2. The two-paragraph prompt to observe how it handles more complex context transfer\n",
    "\n",
    " Each test will generate multiple continuations to show the consistency and variability of the transfer.\n",
    " try for paragraph 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25137831-c706-4f70-b26b-c3140450c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "par1_continued_tokens = transfer_activations(prompt_tokens)\n",
    "for tok in par1_continued_tokens:\n",
    "    print(model.to_str_tokens(tok))\n",
    "\n",
    "# try for paragraph 2\n",
    "par2_continued_tokens = transfer_activations(prompt_with_par1)\n",
    "for tok in par2_continued_tokens:\n",
    "    print(model.to_str_tokens(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88fc66-d64d-4f01-8f9e-88f559f67864",
   "metadata": {},
   "source": [
    " What do you see?\n",
    " You should see that the transfered activations give *some* indicaiton of what the model was planning to write, but that they are far from perfect. It seems like the model can guess the next few words, and can sometimes bootstrap from there, but often fails to.\n",
    " Can we evaluate this more automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70cc36d-9a72-479f-b81a-ec087b20fe3f",
   "metadata": {},
   "source": [
    " # Measuring the quality of the continuations.\n",
    " We use semantic text embeddings to measure how well our continuation ParaScope technique works.\n",
    " Semantic embeddings convert text into high-dimensional vectors that capture meaning - texts with\n",
    " similar meanings will have vectors that point in similar directions (high cosine similarity).\n",
    "\n",
    " Our evaluation process:\n",
    " 1. Encode both the original paragraph and the generated continuation into embedding vectors\n",
    " 2. Calculate cosine similarity between these vectors (ranges from -1 to 1, where 1 = identical meaning)\n",
    " 3. Higher similarity scores indicate that the continuation successfully captured the semantic content\n",
    "    that the model was \"planning\" to write in the original context\n",
    "\n",
    " This helps us quantify whether activation transfer preserves meaningful contextual information\n",
    " or just produces random text that happens to follow the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099b653-c494-45e0-92b5-024db5194927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load evaluation model\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "def calculate_similarities(original_text, continued_tokens):\n",
    "    \"\"\"Calculate cosine similarities between original and continued text.\"\"\"\n",
    "    similarities = []\n",
    "\n",
    "    for cont_tokens in continued_tokens:\n",
    "        cont_text = \"\".join(model.to_str_tokens(cont_tokens))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        if original_text and cont_text:\n",
    "            # [your implementation here]\n",
    "            raise NotImplementedError(\"You need to implement this.\")\n",
    "\n",
    "    return similarities\n",
    "\n",
    "# Calculate similarities for both paragraphs\n",
    "par1_similarities = calculate_similarities(par1_str, par1_continued_tokens)\n",
    "par2_similarities = calculate_similarities(par2_str, par2_continued_tokens)\n",
    "\n",
    "# Create comparison graph\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "box_data = [par1_similarities, par2_similarities]\n",
    "bp = ax.boxplot(box_data)\n",
    "ax.set_xticklabels(['Paragraph 1', 'Paragraph 2'])\n",
    "ax.set_ylabel('Cosine Similarity')\n",
    "ax.set_title('Semantic Similarity: Original vs Continued Text')\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Paragraph 1 mean similarity: {np.mean(par1_similarities):.3f}\")\n",
    "print(f\"Paragraph 2 mean similarity: {np.mean(par2_similarities):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8a4ce-6e5b-4f90-a6d4-0bb9338c375f",
   "metadata": {},
   "source": [
    " ### Bonus: Test which layers are most important for the continuation.\n",
    " We use all the layers, but maybe some are more important than others? Try to change which layers are selected or sweep through them.\n",
    "\n",
    " ### Bonus: (ie: exercises not yet implemented)\n",
    " Try to replicate other parts of the post:\n",
    " [https://link.nicky.pro/parascopes](https://link.nicky.pro/parascopes)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
