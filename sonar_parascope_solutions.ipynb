{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa08d36c-21b7-402b-8155-63db75f665e1",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Section 3: SONAR Parascopes - Trained Probes\n",
    " ============================================\n",
    "\n",
    " While continuation parascopes directly use the model's own generation capabilities,\n",
    " SONAR parascopes take a different approach by learning to map residual streams to\n",
    " text embeddings that can be decoded back to text.\n",
    "\n",
    "![AutoEncoder Map ParaScope](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07873b3421363d38f1cee7649b8bd73fccd43300afd5c2fa.png)\n",
    "\n",
    " The SONAR approach:\n",
    " 1. Train a probe (Linear or MLP) to map from residual stream â†’ SONAR embedding space\n",
    " 2. Use SONAR's decoder to convert embeddings back to text\n",
    " 3. This allows us to extract semantic content without relying on the model's generation\n",
    "\n",
    " Learning objectives:\n",
    " 1. Understand the SONAR text autoencoder and its embedding space\n",
    " 2. Load pre-generated datasets of residual streams and SONAR embeddings\n",
    " 3. Train probes to map between these spaces\n",
    " 4. Evaluate and compare with continuation parascopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250b9ae-3aa1-4a1f-bc00-d5732ff2c42a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Setup and Installation\n",
    "\n",
    " First, install SONAR and other required packages.\n",
    "# %%\n",
    "!pip install -q sonar-space torch torchvision transformer-lens sentence-transformers\n",
    "!pip install -q matplotlib seaborn pandas numpy scikit-learn einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc6401-9af4-44c8-a8e4-ac9301b8bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import einops\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# For the model\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# SONAR imports\n",
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
    "\n",
    "# For evaluation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Disable gradients by default\n",
    "torch.set_grad_enabled(False)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.bfloat16\n",
    "print(f\"Using device: {DEVICE}, dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e9faa-2836-4969-afa5-da28944cb72d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c8aa5-2d47-468b-9bd7-05f76905685b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190e3aa44a484feb8e19c4af4562f3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-3B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", device=DEVICE, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d654ca4-604d-4853-9cc3-a15c00df695e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 1: Generating Training Data\n",
    "\n",
    " Since we want the probe to match \"what the model might output\", we need data of what the model's output might look like.\n",
    " Two-step process: (1) load dataset + generate prompts, (2) use prompts to generate model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534d0d5-9024-4504-b0e1-9bf01a93556a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 1: Generating Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f390be3-0c7e-491c-8bff-34519c11221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac12c3e98bb440185dae419dcaea63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d151bb2c2b645e791a956ca9bb2865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a606cf680d664cb0bd18a62d32542808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a7fd2bfea74c62a369ca0690640be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01d66a617bd482e914adc516759a3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 training examples\n",
      "Example prompts:\n",
      "0: [\"Write a biographical essay about Jane Austen, which explores her advocacy for women's independence and freedom in 19th-century England, compares her influence to that of Thomas Jefferson, and examines the parallels between her life and the American Revolution, including her birth during a pivotal moment in American history and her own quiet yet powerful promotion of the same principles of freedom and self-regulated independence that underpinned the American struggle for independence.\"]\n",
      "1: ['Write a thought-provoking article exploring the importance of play across all ages and species, discussing its biological and spiritual underpinnings, and examining the impact of play deprivation on human development, including its effects on learning, memory, well-being, and childhood obesity, while also delving into the tension between nostalgia and practicality in modern parenting.']\n",
      "2: ['Write a comprehensive article on HIV transmission, which includes the modes of transmission, risk factors, and prevention methods, with approximately 7-10 paragraphs.']\n",
      "3: ['Write a comprehensive guide that explores the complexities of software licensing, including the differences between free and proprietary software licenses, the distinctions between free software, open-source software, and FOSS, and the concepts of copyleft and public domain licensing, along with a detailed explanation of shareware and freeware, with the piece being approximately 5-7 paragraphs long.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def format_prompt(prompt: list[str] | str, system_prompt: str = None) -> list[str]:\n",
    "    def format_prompt_string(prompt: str) -> str:\n",
    "        \"\"\"Format prompt using the model's chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        if system_prompt:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "        return model.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    if isinstance(prompt, str):\n",
    "        return [format_prompt_string(prompt)]\n",
    "    elif isinstance(prompt, list):\n",
    "        return [format_prompt_string(p) for p in prompt]\n",
    "\n",
    "def format_question_string(text: str, max_chars: int = 32000) -> str:\n",
    "    \"\"\"Transform existing text into a generation prompt.\"\"\"\n",
    "    return f\"\"\"Content: {text[:max_chars]}\n",
    "\n",
    "REQUEST: Write a prompt based on the above text, that is a single-paragraph, high-level description. Make the prompt in the format: \"Write a (article/piece/entry), which includes (2-5 topics). The piece should be approximately (many n-paragraphs) long.\"\n",
    "\n",
    "Only provide the prompt, do not write anything else.\"\"\"\n",
    "\n",
    "def format_question(text: list[str] | str, max_chars: int = 4000) -> list[str]:\n",
    "    if isinstance(text, str):\n",
    "        return [format_question_string(text, max_chars)]\n",
    "    elif isinstance(text, list):\n",
    "        return [format_question_string(t, max_chars) for t in text]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid text type: {type(text)}\")\n",
    "\n",
    "# Load dataset and generate training data\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", streaming=True)\n",
    "train_dataloader = DataLoader(dataset.take(4), batch_size=1)\n",
    "# Note: for whatever reason with TransformerLens HookedTransformer,\n",
    "# the generation does not work correctly with batching?\n",
    "# TODO: make it work with batch > 1\n",
    "\n",
    "# Step 1: Generate prompts from texts\n",
    "prompts = []\n",
    "for data in train_dataloader:  # Start with just 4 examples\n",
    "    # Format the meta-prompt properly\n",
    "    meta_prompt = format_question(data[\"text\"])\n",
    "    formatted_prompt = format_prompt(meta_prompt, system_prompt=\"You are a prompt-writing assistant. You are given a text and you need to write a prompt that will generate a response that is similar to the text.\")\n",
    "\n",
    "    # Tokenize and generate\n",
    "    model.tokenizer.pad_token_id = model.tokenizer.eos_token_id\n",
    "    prompt_tokens = model.to_tokens(\n",
    "        formatted_prompt, prepend_bos=False, padding_side=\"left\")\n",
    "    generated_tokens = model.generate(\n",
    "        prompt_tokens,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extract just the generated part\n",
    "    output_tokens = generated_tokens[:, len(prompt_tokens[0]):-1]\n",
    "    generated_prompt = model.to_string(output_tokens)\n",
    "\n",
    "    prompts.extend([t.strip() for t in generated_prompt])\n",
    "\n",
    "print(f\"Generated {len(prompts)} training examples\")\n",
    "print(f\"Example prompts:\")\n",
    "[print(f\"{i}: {[p]}\") for i, p in enumerate(prompts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412748a0-8227-439d-ad46-581befa7bc3e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 2: Generate Model Outputs from Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3723299-605e-4340-a36b-16a1fe3d0ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output 1/4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae67b8d747a4bdeb0f9f16a33086ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output 2/4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253e6e0419824b8390e0a7d22464969a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output 3/4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee4cf9c5b6a48fea0e067de31d1336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output 4/4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff72c31853342b7b068306913704651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 model outputs\n",
      "Example outputs:\n",
      "0: ['Jane Austen, the celebrated English novelist, is often regarded as a chronicler of the social lives of the English gentry in the late 18th and early 19th centuries. However, beneath her witty and insightful prose lies a complex and multifaceted individual who advocated for women\\'s independence and freedom in a society that was deeply patriarchal and restrictive. In this essay, we will explore Austen\\'s advocacy for women\\'s independence, compare her influence to that of Thomas Jefferson, and examine the parallels between her life and the American Revolution.\\n\\nAusten was born in 1775, during a pivotal moment in American history. The American colonies had just declared their independence from Great Britain, and the spirit of revolution was spreading rapidly across the Atlantic. Austen\\'s own family was influenced by these events, with her father, Reverend Austen, being a staunch supporter of the American cause. This exposure to the ideals of freedom and self-governance would shape Austen\\'s own views on women\\'s rights and her critique of the social conventions that limited women\\'s autonomy.\\n\\nAusten\\'s novels, such as \"Pride and Prejudice\" and \"Sense and Sensibility,\" offer a scathing critique of the social norms that governed women\\'s lives. Her female characters, like Elizabeth Bennet and Elinor Dashwood, are depicted as intelligent, independent, and strong-willed individuals who refuse to be bound by the conventions of marriage and societal expectations. Austen\\'s portrayal']\n",
      "1: [\"**The Universality of Play: Unlocking its Biological, Spiritual, and Social Significance**\\n\\nPlay is an integral part of the human experience, transcending age, species, and culture. From the carefree romps of childhood to the creative pursuits of adulthood, play has been an essential component of our lives, fostering growth, connection, and joy. Yet, despite its ubiquity, play has become an increasingly rare and curated activity, often relegated to the margins of our busy lives. In this article, we'll delve into the biological, spiritual, and social significance of play, exploring its impact on human development, and examining the tension between nostalgia and practicality in modern parenting.\\n\\n**Biological Roots of Play**\\n\\nPlay is deeply ingrained in our biology, serving as a fundamental aspect of human development. From infancy to old age, play has played a critical role in shaping our physical, cognitive, and emotional well-being. Research has shown that play stimulates the release of endorphins, dopamine, and serotonin, which regulate mood, motivation, and social behavior. Play also enhances cognitive skills such as problem-solving, creativity, and spatial reasoning, laying the foundation for academic and professional success.\\n\\nMoreover, play has been shown to have a profound impact on our physical health, reducing stress, anxiety, and blood pressure, while increasing flexibility, coordination, and overall well-being. The importance of play in physical development cannot be overstated, as it fosters motor skills, balance, and spatial awareness,\"]\n",
      "2: [\"**Understanding HIV Transmission: Modes, Risk Factors, and Prevention Methods**\\n\\nHuman immunodeficiency virus (HIV) is a viral infection that attacks the body's immune system, making it harder for the body to fight off infections and diseases. HIV is transmitted through various modes, and understanding these modes, risk factors, and prevention methods is crucial for individuals to protect themselves and others from contracting the virus. In this article, we will delve into the different modes of HIV transmission, risk factors, and effective prevention methods.\\n\\n**Modes of HIV Transmission**\\n\\nHIV can be transmitted through various modes, including:\\n\\n1. **Unprotected sex**: HIV can be transmitted through vaginal, anal, or oral sex with an infected person, especially if the infected person has a high viral load.\\n2. **Sharing needles or syringes**: HIV can be transmitted through sharing needles or syringes with an infected person, especially if the infected person has not been tested for HIV.\\n3. **Mother-to-child transmission**: HIV can be transmitted from an infected mother to her child during pregnancy, childbirth, or breastfeeding.\\n4. **Blood transfusions**: HIV can be transmitted through blood transfusions from an infected donor, although this is rare in countries with strict blood screening regulations.\\n5. **Occupational exposure**: HIV can be transmitted to healthcare workers through occupational exposure, such as needle sticks or other sharp objects.\\n\\n**Risk Factors**\\n\\nCertain individuals are at a higher risk of contracting HIV, including:\\n\\n1. **Un\"]\n",
      "3: [\"**A Comprehensive Guide to Software Licensing: Navigating the Complexities of Free and Proprietary Software**\\n\\nSoftware licensing is a critical aspect of the software development and distribution process, governing the terms and conditions under which software can be used, modified, and distributed. At its core, software licensing is a contract between the software developer and the user, outlining the rights and responsibilities associated with the software. In this guide, we'll delve into the complexities of software licensing, exploring the differences between free and proprietary software licenses, free software, open-source software, and FOSS, as well as copyleft and public domain licensing.\\n\\n**Free and Proprietary Software Licenses**\\n\\nThe primary distinction between free and proprietary software licenses lies in their licensing terms. Proprietary software licenses, such as those used by Microsoft and Apple, restrict users from modifying, sharing, or distributing the software without permission from the developer. In contrast, free software licenses, like the Mozilla Public License (MPL) and the GNU General Public License (GPL), allow users to modify, share, and distribute the software freely, while still requiring them to disclose any changes made to the original code. Free software licenses often include copyleft provisions, which ensure that any derivative works must also be released under the same license.\\n\\n**Free Software, Open-Source Software, and FOSS**\\n\\nThe terms free software, open-source software (OSS), and free and open-source software (FOSS) are often used interchangeably, but\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "model_outputs = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating output {i+1}/{len(prompts)}...\")\n",
    "\n",
    "    # Format and tokenize the prompt\n",
    "    formatted_prompt = format_prompt(prompt)\n",
    "    prompt_tokens = model.to_tokens(\n",
    "        formatted_prompt, prepend_bos=False, padding_side=\"left\")\n",
    "\n",
    "    # Generate response\n",
    "    output_tokens = model.generate(\n",
    "        prompt_tokens,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract just the generated part\n",
    "    generated_tokens = output_tokens[:, len(prompt_tokens[0]):]\n",
    "    generated_text = model.to_string(generated_tokens)[0]\n",
    "\n",
    "    model_outputs.append(generated_text.strip())\n",
    "\n",
    "print(f\"Generated {len(model_outputs)} model outputs\")\n",
    "print(f\"Example outputs:\")\n",
    "[print(f\"{i}: {[o]}\") for i, o in enumerate(model_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e787eb-8a06-456b-8c53-15996654ecff",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 3: Loading Pre-made Data\n",
    " The above takes too long to run, so we'll load pre-made data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9a2b9-e0eb-4343-94a4-5d6a10cececc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: [0]\n",
      "prompt: ['Write a Chapter titled \"The Quiet Revolution of Jane Austen\\'s Independence\", exploring the theme of Austen\\'s advocacy for female independence and the principles of freedom and self-regulation in her novels, contrasting her quiet yet powerful impact with the more explosive consequences of American and French Revolutions of her time.']\n",
      "completion: [\"**Chapter 7: The Quiet Revolution of Jane Austen's Independence**\\n\\nIn an era where the French Revolution's loud declarations of liberty, equality, and fraternity had sparked fervent discussions and bloody conflicts across Europe, a quieter revolution was unfolding in the English countryside. Jane Austen, a writer often overlooked for her time, was quietly advocating for female independence and the principles of freedom and self-regulation in her novels. While the likes of Maximilien Robespierre and George Washington were railing against tyranny and demanding radical change, Austen was working to empower women through subtle yet profound literary endorsements.\\n\\nAusten's writing career spanned the late 18th and early 19th centuries, a period marked by significant social and economic changes. The Industrial Revolution had created new opportunities for women to enter the workforce, but the constraints of a patriarchal society ensured that their choices and ambitions were often limited. Austen's novels, particularly those featuring strong female protagonists like Elizabeth Bennet in _Pride and Prejudice_ and Elinor Dashwood in _Sense and Sensibility_, proposed a radical alternative to the traditional notion of femininity.\\n\\nIn _Pride and Prejudice_, Austen subverts the conventional notion of a woman's role in society by depicting a heroine who is intelligent, witty, and determined to marry for love rather than convenience. Elizabeth Bennet's opposition to Mr. Collins's proposal, with its implication of a mercenary marriage, is a pointed critique of the social pressures that forced women into unwanted alliances. Austen shows that a woman's happiness and fulfillment depend on her own agency and self-respect, not on the favor of a man.\\n\\nSimilarly, in _Sense and Sensibility_, Austen explores the theme of female independence through the characters of Elinor and Marianne Dashwood. Elinor's cautious and practical approach to love and relationships serves as a foil to Marianne's emotional and expressive nature. While Marianne's enthusiasm and passion are eventually tempered by the demands of reality, Elinor's steady determination to make her own way in life remains a powerful symbol of female autonomy.\\n\\nAusten's advocacy for female independence was not limited to her novels. In her letters and personal life, she demonstrated a commitment to the values of self-regulation and personal responsibility. A woman of modest means and limited social connections, Austen was aware of the restrictions placed on her by her sex and her station. Yet, she refused to be constrained by these limitations, instead cultivating a spirit of independence and self-reliance that informed her writing and shaped her relationships.\\n\\nIn contrast to the more explosive consequences of the American and French Revolutions, which aimed to overthrow entire systems of power and social order, Austen's quiet revolution was more subtle and insidious. Her novels and letters, written in a tone of gentle persuasion rather than revolutionary fervor, seeped into the social consciousness like a gentle stream, influencing generations of women to come. By depicting strong, independent female characters and advocating for the value of personal autonomy, Austen contributed to a gradual shift in societal attitudes, one that recognized the agency and dignity of women.\\n\\nAusten's legacy is a testament to the power of quiet, incremental change. Her novels, written during a time of great social upheaval, offer a vision of a more just and equitable society, one in which women are valued for their own worth and agency rather than being defined by their relationships with men. As we reflect on the quiet revolution of Jane Austen's independence, we are reminded that true change often begins with small, subtle shifts in our understanding of the world and our place within it.\\n\\nIn the end, Austen's legacy serves as a reminder that revolution is not always loud or dramatic. Sometimes, the most profound transformations occur through quiet, steady efforts, as individuals and societies work together to create a more just and equitable world. As we celebrate the quiet revolution of Jane Austen's independence, we honor a woman who, through her writing and her life, has inspired generations to stand up for their own freedom and dignity.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nickypro/fineweb-llama3b-regen\", split=\"train\")\n",
    "\n",
    "[print(f\"{k}: {[v]}\") for k,v in dataset[0].items()]\n",
    "# {id: 0, prompt: \"...\", completion: \"...\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b154a8-1060-439b-90c0-d97852e0e7a3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 4: Splitting into Sections\n",
    " As we want to predict that the \"next section\" of the text will say, we need to split the text into sections.\n",
    " I choose to split the text by paragraphs, but it is also reasonable to split it by sentences or something similar.\n",
    " For the data here, we want data of the form:\n",
    " [prompt, section_1, section_2, section_3, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c8547-14e1-48ba-9360-816442125fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/1000000 [00:00<02:10, 7633.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Jul 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a Chapter titled \"The Quiet Revolution of Jane Austen\\'s Independence\", exploring the theme of Austen\\'s advocacy for female independence and the principles of freedom and self-regulation in her novels, contrasting her quiet yet powerful impact with the more explosive consequences of American and French Revolutions of her time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'], \"**Chapter 7: The Quiet Revolution of Jane Austen's Independence**\\n\\n\", \"In an era where the French Revolution's loud declarations of liberty, equality, and fraternity had sparked fervent discussions and bloody conflicts across Europe, a quieter revolution was unfolding in the English countryside. Jane Austen, a writer often overlooked for her time, was quietly advocating for female independence and the principles of freedom and self-regulation in her novels. While the likes of Maximilien Robespierre and George Washington were railing against tyranny and demanding radical change, Austen was working to empower women through subtle yet profound literary endorsements.\\n\\n\", \"Austen's writing career spanned the late 18th and early 19th centuries, a period marked by significant social and economic changes. The Industrial Revolution had created new opportunities for women to enter the workforce, but the constraints of a patriarchal society ensured that their choices and ambitions were often limited. Austen's novels, particularly those featuring strong female protagonists like Elizabeth Bennet in _Pride and Prejudice_ and Elinor Dashwood in _Sense and Sensibility_, proposed a radical alternative to the traditional notion of femininity.\\n\\n\", \"In _Pride and Prejudice_, Austen subverts the conventional notion of a woman's role in society by depicting a heroine who is intelligent, witty, and determined to marry for love rather than convenience. Elizabeth Bennet's opposition to Mr. Collins's proposal, with its implication of a mercenary marriage, is a pointed critique of the social pressures that forced women into unwanted alliances. Austen shows that a woman's happiness and fulfillment depend on her own agency and self-respect, not on the favor of a man.\\n\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def split_text_into_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a block of text into paragraphs.\n",
    "    Paragraphs are separated by two or more newlines.\n",
    "    The convention we choose is to have the newlines stored at the end of each paragraph.\n",
    "    The list should combine to give the original text.\n",
    "    \"\"\"\n",
    "    paragraphs = [p+\"\\n\\n\" for p in text.split('\\n\\n')]\n",
    "    return paragraphs\n",
    "\n",
    "def split_dataset_prompt_and_sections(dataset) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    For each example in the dataset, keeps the prompt as a single string,\n",
    "    and splits the 'completion' field into paragraphs.\n",
    "    Returns a list of lists:\n",
    "    [ [prompt, section_1, section_2, ...], ... ]\n",
    "    \"\"\"\n",
    "    split_data = []\n",
    "    for i, example in enumerate(tqdm(dataset)):\n",
    "        prompt = format_prompt(example[\"prompt\"])\n",
    "        completion = example[\"completion\"]\n",
    "        completion_paragraphs = split_text_into_paragraphs(completion)\n",
    "        # The first element is the prompt (as a single string), then the completion paragraphs\n",
    "        split_text = [prompt] + completion_paragraphs\n",
    "        split_data.append({\"id\": i, \"split_text\": split_text})\n",
    "\n",
    "        if i > 100:\n",
    "            # Lazy mode: only do 100 examples\n",
    "            break\n",
    "    return split_data\n",
    "\n",
    "# Example usage:\n",
    "split_sections = split_dataset_prompt_and_sections(dataset)\n",
    "print(split_sections[0][\"split_text\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eec8d0-cf9f-440e-a625-b42753b926c6",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Alternatively, I don't like waiting for this either, so we can just load the pre-split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cde676-e841-4c26-8239-98312d7efb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'split_text': ['<|start_header_id|>user<|end_header_id|>\\nWrite a Chapter titled \"The Quiet Revolution of Jane Austen\\'s Independence\", exploring the theme of Austen\\'s advocacy for female independence and the principles of freedom and self-regulation in her novels, contrasting her quiet yet powerful impact with the more explosive consequences of American and French Revolutions of her time.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n', \"**Chapter 7: The Quiet Revolution of Jane Austen's Independence**\\n\\n\", \"In an era where the French Revolution's loud declarations of liberty, equality, and fraternity had sparked fervent discussions and bloody conflicts across Europe, a quieter revolution was unfolding in the English countryside. Jane Austen, a writer often overlooked for her time, was quietly advocating for female independence and the principles of freedom and self-regulation in her novels. While the likes of Maximilien Robespierre and George Washington were railing against tyranny and demanding radical change, Austen was working to empower women through subtle yet profound literary endorsements.\\n\\n\", \"Austen's writing career spanned the late 18th and early 19th centuries, a period marked by significant social and economic changes. The Industrial Revolution had created new opportunities for women to enter the workforce, but the constraints of a patriarchal society ensured that their choices and ambitions were often limited. Austen's novels, particularly those featuring strong female protagonists like Elizabeth Bennet in _Pride and Prejudice_ and Elinor Dashwood in _Sense and Sensibility_, proposed a radical alternative to the traditional notion of femininity.\\n\\n\", \"In _Pride and Prejudice_, Austen subverts the conventional notion of a woman's role in society by depicting a heroine who is intelligent, witty, and determined to marry for love rather than convenience. Elizabeth Bennet's opposition to Mr. Collins's proposal, with its implication of a mercenary marriage, is a pointed critique of the social pressures that forced women into unwanted alliances. Austen shows that a woman's happiness and fulfillment depend on her own agency and self-respect, not on the favor of a man.\\n\\n\", \"Similarly, in _Sense and Sensibility_, Austen explores the theme of female independence through the characters of Elinor and Marianne Dashwood. Elinor's cautious and practical approach to love and relationships serves as a foil to Marianne's emotional and expressive nature. While Marianne's enthusiasm and passion are eventually tempered by the demands of reality, Elinor's steady determination to make her own way in life remains a powerful symbol of female autonomy.\\n\\n\", \"Austen's advocacy for female independence was not limited to her novels. In her letters and personal life, she demonstrated a commitment to the values of self-regulation and personal responsibility. A woman of modest means and limited social connections, Austen was aware of the restrictions placed on her by her sex and her station. Yet, she refused to be constrained by these limitations, instead cultivating a spirit of independence and self-reliance that informed her writing and shaped her relationships.\\n\\n\", \"In contrast to the more explosive consequences of the American and French Revolutions, which aimed to overthrow entire systems of power and social order, Austen's quiet revolution was more subtle and insidious. Her novels and letters, written in a tone of gentle persuasion rather than revolutionary fervor, seeped into the social consciousness like a gentle stream, influencing generations of women to come. By depicting strong, independent female characters and advocating for the value of personal autonomy, Austen contributed to a gradual shift in societal attitudes, one that recognized the agency and dignity of women.\\n\\n\", \"Austen's legacy is a testament to the power of quiet, incremental change. Her novels, written during a time of great social upheaval, offer a vision of a more just and equitable society, one in which women are valued for their own worth and agency rather than being defined by their relationships with men. As we reflect on the quiet revolution of Jane Austen's independence, we are reminded that true change often begins with small, subtle shifts in our understanding of the world and our place within it.\\n\\n\", \"In the end, Austen's legacy serves as a reminder that revolution is not always loud or dramatic. Sometimes, the most profound transformations occur through quiet, steady efforts, as individuals and societies work together to create a more just and equitable world. As we celebrate the quiet revolution of Jane Austen's independence, we honor a woman who, through her writing and her life, has inspired generations to stand up for their own freedom and dignity.\"]}\n"
     ]
    }
   ],
   "source": [
    "split_sections = load_dataset(\"nickypro/fineweb-llama3b-regen-split-formatted\", split=\"train\")\n",
    "\n",
    "print(split_sections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a4361-e68d-4ccc-90db-06d9c5cce622",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 2: Loading Residual Streams and Embeddings\n",
    "\n",
    " We need to load pre-generated data containing:\n",
    " - Residual stream activations from language models\n",
    " - Corresponding SONAR embeddings of the paragraphs\n",
    " - The actual paragraph text (we got this above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816624c9-46a7-4713-964a-b6cf12910ba9",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Part 1: Loading Residual Streams\n",
    " So the transformer model has blocks of:\n",
    " [resid_pre] -> Attention -> [resid_mid] -> MLP -> [resid_post] == [resid_pre_n+1]\n",
    " I have only tested things so far with residual difference\n",
    " I.e: resid_mid = resid_pre + attn_results == resid_pre + resid_mid_diff == resid_pre + (resid_mid - resid_pre)\n",
    " I find it more consistent to calculate (option 1):\n",
    " * resid_mid_diff = resid_mid - resid_pre\n",
    " * resid_post_diff = resid_post - resid_mid\n",
    " alternatively, we could do (option 2):\n",
    " * resid_layer_diff = resid_post - resid_pre\n",
    " which should also work.\n",
    " My testing so far has used option 1, mostly so I can somewhat see where it is easier to extract the information.\n",
    " I suspect that the probes could work also using the basline activations [resid_mid, resid_post] or even just [resid_post], but I haven't tested it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c53bd6-c28c-45bf-8ade-d559448dfd16",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " We make hooks for storing activations of the residual stream at the correct positions. That is, the final token of each \"section\" of tokens.\n",
    " For now, we just save [resid_pre, resid_mid, resid_post] at the end of each section.\n",
    " Later we can process it however we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485d3e3-084a-4181-a13e-3d562d5666b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 71]) [torch.Size([1, 15]), torch.Size([1, 105]), torch.Size([1, 111]), torch.Size([1, 107]), torch.Size([1, 92]), torch.Size([1, 95]), torch.Size([1, 113]), torch.Size([1, 102]), torch.Size([1, 88])]\n",
      "['\\n', '**\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.']\n",
      "blocks.0.hook_resid_pre torch.Size([1, 10, 3072])\n"
     ]
    }
   ],
   "source": [
    "def get_act_data(split_text, act_types=None, verbose=False):\n",
    "    # choose which residual data to collect\n",
    "    if act_types is None:\n",
    "        act_types = [\"hook_resid_pre\", \"hook_resid_mid\", \"hook_resid_post\"]\n",
    "    hook_names = [\n",
    "        f\"blocks.{i}.{resid_type}\"\n",
    "            for i in range(model.cfg.n_layers)\n",
    "            for resid_type in act_types\n",
    "    ]\n",
    "\n",
    "    # get prompt vs output separately\n",
    "    prompt = split_text[0]\n",
    "    output = split_text[1:]\n",
    "\n",
    "    # Tokenize the prompt and output correctly\n",
    "    prompt_tokens =  model.to_tokens(prompt, prepend_bos=True)\n",
    "    output_tokens = [model.to_tokens(o, prepend_bos=False) for o in output]\n",
    "    if verbose:\n",
    "        print(prompt_tokens.shape, [o.shape for o in output_tokens])\n",
    "    all_tokens = torch.cat([prompt_tokens, torch.cat(output_tokens, dim=1)], dim=1)\n",
    "\n",
    "    # Get the indices of the residual streams that we want to store\n",
    "    # Ie: last token of each section, usually \"\\n\\n\"\n",
    "    final_indices_rel = [\n",
    "        prompt_tokens.shape[-1],\n",
    "        *[ o.shape[-1] for o in output_tokens ]\n",
    "    ]\n",
    "    final_indices_abs = np.cumsum(final_indices_rel) - 1\n",
    "\n",
    "    # check the tokens are actually the newline ones\n",
    "    if verbose:\n",
    "        print(model.to_str_tokens(all_tokens[:, final_indices_abs]))\n",
    "\n",
    "    # Create hooks to store activations of only the correct residual streams\n",
    "    act_data = {}\n",
    "    def store_act(act, hook):\n",
    "        act_data[hook.name] = act[..., final_indices_abs, :]\n",
    "    hook_list = [(name, store_act) for name in hook_names]\n",
    "\n",
    "    # Run model and store activations\n",
    "    with model.hooks(fwd_hooks=hook_list):\n",
    "        model.forward(all_tokens)\n",
    "\n",
    "    # Print some info\n",
    "    if verbose:\n",
    "        for k, v in act_data.items():\n",
    "            print(k, v.shape)\n",
    "            break\n",
    "\n",
    "    return act_data\n",
    "\n",
    "act_data = get_act_data(split_sections[0][\"split_text\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394ebb1-f4fe-4a5c-9cbf-93f70101f8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 71]) [torch.Size([1, 15]), torch.Size([1, 105]), torch.Size([1, 111]), torch.Size([1, 107]), torch.Size([1, 92]), torch.Size([1, 95]), torch.Size([1, 113]), torch.Size([1, 102]), torch.Size([1, 88])]\n",
      "['\\n', '**\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.\\n\\n', '.']\n",
      "blocks.0.hook_resid_pre torch.Size([1, 10, 3072])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Now we actually save residual stream diff data, as describe as \"option 1\" above.\n",
    "\n",
    "def get_resid_diff_data(split_text, act_types=None, verbose=False):\n",
    "    act_data = get_act_data(split_text, act_types, verbose)\n",
    "\n",
    "    # Calculate the difference between the residual streams\n",
    "    act_data[\"resid_mid_diff\"]  = [\n",
    "        act_data[f\"blocks.{i}.hook_resid_mid\"] - act_data[f\"blocks.{i}.hook_resid_pre\"]\n",
    "        for i in range(model.cfg.n_layers)\n",
    "    ]\n",
    "    act_data[\"resid_post_diff\"] = [\n",
    "        act_data[f\"blocks.{i}.hook_resid_post\"] - act_data[f\"blocks.{i}.hook_resid_mid\"]\n",
    "        for i in range(model.cfg.n_layers)\n",
    "    ]\n",
    "\n",
    "    # Concatenate the data into a single tensor\n",
    "    # I previously did this as [batch==1, layers, tokens, d_model]\n",
    "    # where layers is ordered as [resid_pre_0] + \\\n",
    "    # [mid_diff_0, post_diff_0, mid_diff_1, post_diff_1, ...]\n",
    "    # So for consistency, we store it this way.\n",
    "    full_act_data = [act_data[\"blocks.0.hook_resid_pre\"]]\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        full_act_data.append(act_data[\"resid_mid_diff\"][i])\n",
    "        full_act_data.append(act_data[\"resid_post_diff\"][i])\n",
    "\n",
    "    return torch.cat(full_act_data, dim=0).unsqueeze(0)\n",
    "\n",
    "resid_diff_data = get_resid_diff_data(split_sections[0][\"split_text\"], verbose=True)\n",
    "resid_diff_data.shape\n",
    "\n",
    "# note: I kinda wish I stored things as raw residuals instead of diffs, since it's easier to convert to diffs later.\n",
    "\n",
    "# We are now finished with the model so we can remove it from memory.\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c207a89-d47a-47f7-9704-84cee87f0dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'split': ['<|start_header_id|>user<|end_header_id|>\\nWrite a news feed entry titled \"ACA Repeal Bill Exposed: A Threat to Women\\'s Health\", which includes topics to cover: the potential consequences of the ACA repeal bill on women\\'s health, the opposition from various groups, and the role of the Republican-controlled Congress in shaping the future of healthcare. The entry should be approximately 2-3 paragraphs in length.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n', \"**ACA Repeal Bill Exposed: A Threat to Women's Health**\\n\\n\", \"A recently leaked draft of the proposed American Health Care Act (AHCA) reveals a catastrophic threat to women's health, sparking widespread opposition from healthcare professionals, advocacy groups, and lawmakers alike. The bill, which aims to repeal and replace the Affordable Care Act (ACA), contains provisions that could lead to significant increases in healthcare costs, reduced access to reproductive health services, and increased risk of maternal mortality. Specifically, the bill would eliminate funding for the Preventive Care Women's Rule, which requires insurance plans to cover certain preventive services, including birth control and Pap smears, without cost-sharing. This move would disproportionately affect low-income women, who rely on these services to maintain their health and well-being.\\n\\n\", 'The American College of Obstetricians and Gynecologists (ACOG) and the Planned Parenthood Federation of America (PPFA) have already spoken out against the bill, warning that its repeal would lead to \"disastrous consequences for women\\'s health.\" Other groups, including the National Organization for Women (NOW) and the American Cancer Society, have also voiced their opposition. These organizations argue that the bill\\'s provisions would not only harm women\\'s health but also undermine the economic stability of women, who are the primary breadwinners in many households. As the Republican-controlled Congress prepares to vote on the bill, it remains to be seen whether lawmakers will listen to the warnings of their constituents and the expertise of healthcare professionals.\\n\\n', \"The fate of the ACA repeal bill hangs in the balance, and the impact on women's health is a pressing concern. As lawmakers consider the bill's provisions, they must weigh the interests of the insurance industry and corporate donors against the well-being of the American people. The American people deserve a healthcare system that prioritizes prevention, equality, and access to essential services, not just profit and partisan politics. The outcome of this vote will have far-reaching consequences for the health and well-being of millions of Americans, particularly women.\"], 'indices': [86, 102, 242, 386]}\n",
      "Loaded tensor shape: torch.Size([1, 57, 4, 3072])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Now we load the pre-computed residual stream diffs.\n",
    "\n",
    "\n",
    "# Download and load a specific file (there are total 100 files, 000-099)\n",
    "# Note these are slightly older files, so these match up with a different dataset.\n",
    "\n",
    "split_sections = load_dataset(\"nickypro/llama-3b-split\", split=\"train\")\n",
    "print(split_sections[0])\n",
    "\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nickypro/llama-3b-residuals\",\n",
    "    filename=\"res_data_000.pt\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Load the tensor\n",
    "tensor_data = torch.load(file_path)\n",
    "print(f\"Loaded tensor shape: {tensor_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8bb69-9b78-4376-9c21-8fe81caffe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# ## Creating the embeddings.\n",
    "# Now we use SONAR to create embeddings for each paragraph.\n",
    "# We use the `sonar.inference_pipelines.text.TextToEmbeddingModelPipeline` class to create the embeddings.\n",
    "# We do this for all the paragraphs in the dataset.\n",
    "\n",
    "# Note again we use a slightly older dataset because I haven't uploaded the new ones to huggingface yet.\n",
    "split_sections = load_dataset(\"nickypro/llama-3b-split\", split=\"train\")\n",
    "print(len(split_sections[0]['split']))\n",
    "\n",
    "text2vec = TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    ")\n",
    "\n",
    "vec2text = EmbeddingToTextModelPipeline(\n",
    "    decoder=\"text_sonar_basic_decoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33db89e-9a2d-42ff-9e69-3c8faeba787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:   ['<|start_header_id|>user<|end_header_id|>\\nWrite a news feed entry titled \"ACA Repeal Bill Exposed: A Threat to Women\\'s Health\", which includes topics to cover: the potential consequences of the ACA repeal bill on women\\'s health, the opposition from various groups, and the role of the Republican-controlled Congress in shaping the future of healthcare. The entry should be approximately 2-3 paragraphs in length.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n']\n",
      "predicted:  ['Write a newsfeed entitled \"ACA Repeal Bill Exposed: A Threat to Women\\'s Health\", which contains topics such as: the potential impact of the ACA Repeal Bill on women\\'s health, the opposition from different groups, and the role of the Republican-controlled Congress in shaping the future of health care.']\n",
      "original:   [\"**ACA Repeal Bill Exposed: A Threat to Women's Health**\\n\\n\"]\n",
      "predicted:  [\"**ACA Repeal Bill Exposed: A Threat to Women's Health**\"]\n",
      "original:   [\"A recently leaked draft of the proposed American Health Care Act (AHCA) reveals a catastrophic threat to women's health, sparking widespread opposition from healthcare professionals, advocacy groups, and lawmakers alike. The bill, which aims to repeal and replace the Affordable Care Act (ACA), contains provisions that could lead to significant increases in healthcare costs, reduced access to reproductive health services, and increased risk of maternal mortality. Specifically, the bill would eliminate funding for the Preventive Care Women's Rule, which requires insurance plans to cover certain preventive services, including birth control and Pap smears, without cost-sharing. This move would disproportionately affect low-income women, who rely on these services to maintain their health and well-being.\\n\\n\"]\n",
      "predicted:  [\"A recently unveiled proposed bill by the American Health Care Act (AHCA) poses a devastating threat to women's health, causing widespread opposition from health care professionals, advocates, and lawmakers. The bill, which aims to repeal the Affordable Care Act (ACA), includes provisions that would significantly reduce health care costs, increase access to preventive health care services, and reduce maternal mortality rates. In particular, the bill would require changes to the Affordable Care Act, which would prohibit the provision of prenatal care services, including prescription-only health care, to cover certain expenses.\"]\n",
      "original:   ['The American College of Obstetricians and Gynecologists (ACOG) and the Planned Parenthood Federation of America (PPFA) have already spoken out against the bill, warning that its repeal would lead to \"disastrous consequences for women\\'s health.\" Other groups, including the National Organization for Women (NOW) and the American Cancer Society, have also voiced their opposition. These organizations argue that the bill\\'s provisions would not only harm women\\'s health but also undermine the economic stability of women, who are the primary breadwinners in many households. As the Republican-controlled Congress prepares to vote on the bill, it remains to be seen whether lawmakers will listen to the warnings of their constituents and the expertise of healthcare professionals.\\n\\n']\n",
      "predicted:  ['The American College of Obstetricians and Gynecologists (ACOG) and the Planned Parenthood Federation of America (PPFA) have already spoken out against the bill, warning that its elimination would have adverse effects on \"women\\'s health\". Other groups, including the National Organization for Women (NOW) and the American Cancer Society, have also opposed the proposals. These proposals claim that not only will women\\'s health impair the economic integrity of many women, but that the family will be the first to be taxed in the United States.']\n",
      "original:   [\"The fate of the ACA repeal bill hangs in the balance, and the impact on women's health is a pressing concern. As lawmakers consider the bill's provisions, they must weigh the interests of the insurance industry and corporate donors against the well-being of the American people. The American people deserve a healthcare system that prioritizes prevention, equality, and access to essential services, not just profit and partisan politics. The outcome of this vote will have far-reaching consequences for the health and well-being of millions of Americans, particularly women.\"]\n",
      "predicted:  [\"The fate of the ACA repeal bill is in question, and the impact on women's health is grave. While legislators are considering the implications of the bill, they must weigh the interests of insurance companies and corporate providers against the welfare of the American people. Americans deserve the health care system that gives priority to prevention, equality, and access to essential services, rather than a pro-choice policy. The health outcomes of this will have a profound impact on the health of millions of Americans, particularly women and girls.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% We try to get the embeddings, and also compare some examples of how well it decodes.\n",
    "\n",
    "embeds = []\n",
    "\n",
    "for i, example in enumerate(tqdm(split_sections)):\n",
    "    _id = example[\"id\"]\n",
    "    split_texts = example[\"split\"]\n",
    "    embeddings = text2vec.predict(split_texts, source_lang=\"eng_Latn\")\n",
    "    embeds.append(embeddings)\n",
    "    print(embeddings.shape)\n",
    "    decoded_texts = vec2text.predict(embeddings, target_lang=\"eng_Latn\")\n",
    "    for j, t in enumerate(split_texts):\n",
    "        print(\"original:  \", [t])\n",
    "        print(\"predicted: \", [decoded_texts[j]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0414c4-0b47-48f4-b75f-b2e37aee3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "original:  [\"**ACA Repeal Bill Exposed: A Threat to Women's Health**\\n\\n\"]\n",
      "predicted: [\"**ACA Repeal Bill Exposed: A Threat to Women's Health**\"]\n"
     ]
    }
   ],
   "source": [
    "# %% alternatively, we can yet again use the pre-computed embeddings.\n",
    "\n",
    "embeds = torch.load(hf_hub_download(\n",
    "    repo_id=\"nickypro/llama-3b-embeds\",\n",
    "    filename=\"embeds_000.pt\",\n",
    "    repo_type=\"dataset\"\n",
    "))\n",
    "print(embeds[0].shape)\n",
    "\n",
    "for i, embed in enumerate(embeds):\n",
    "    decoded_texts = vec2text.predict(embed.to(DTYPE), target_lang=\"eng_Latn\")\n",
    "    print(f\"original:  {[split_sections[i]['split'][1]]}\")\n",
    "    print(f\"predicted: {[decoded_texts[0]]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c447c-3f0d-4fe5-a173-53a624262d0a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Now we have shown how to get the data, we can remove the text2vec pipeline from memory. and instead use the pre-computed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c7890-8825-4c17-8d8e-a3469f661225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "del text2vec\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a973d52-ecc8-4074-9539-ac895cf54273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0\n",
      "torch.Size([4, 57, 3072])\n",
      "torch.Size([4, 1024])\n",
      "['<|start_header_id|>user<|end_header_id|>\\nWrite a news feed entry titled \"ACA Repeal Bill Exposed: A Threat to Women\\'s Health\", which includes topics to cover: the potential consequences of the ACA repeal bill on women\\'s health, the opposition from various groups, and the role of the Republican-controlled Congress in shaping the future of healthcare. The entry should be approximately 2-3 paragraphs in length.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n']\n",
      "[\"**ACA Repeal Bill Exposed: A Threat to Women's Health**\\n\\n\"]\n",
      "[\"A recently leaked draft of the proposed American Health Care Act (AHCA) reveals a catastrophic threat to women's health, sparking widespread opposition from healthcare professionals, advocacy groups, and lawmakers alike. The bill, which aims to repeal and replace the Affordable Care Act (ACA), contains provisions that could lead to significant increases in healthcare costs, reduced access to reproductive health services, and increased risk of maternal mortality. Specifically, the bill would eliminate funding for the Preventive Care Women's Rule, which requires insurance plans to cover certain preventive services, including birth control and Pap smears, without cost-sharing. This move would disproportionately affect low-income women, who rely on these services to maintain their health and well-being.\\n\\n\"]\n",
      "['The American College of Obstetricians and Gynecologists (ACOG) and the Planned Parenthood Federation of America (PPFA) have already spoken out against the bill, warning that its repeal would lead to \"disastrous consequences for women\\'s health.\" Other groups, including the National Organization for Women (NOW) and the American Cancer Society, have also voiced their opposition. These organizations argue that the bill\\'s provisions would not only harm women\\'s health but also undermine the economic stability of women, who are the primary breadwinners in many households. As the Republican-controlled Congress prepares to vote on the bill, it remains to be seen whether lawmakers will listen to the warnings of their constituents and the expertise of healthcare professionals.\\n\\n']\n",
      "[\"The fate of the ACA repeal bill hangs in the balance, and the impact on women's health is a pressing concern. As lawmakers consider the bill's provisions, they must weigh the interests of the insurance industry and corporate donors against the well-being of the American people. The American people deserve a healthcare system that prioritizes prevention, equality, and access to essential services, not just profit and partisan politics. The outcome of this vote will have far-reaching consequences for the health and well-being of millions of Americans, particularly women.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Now we just load all data.\n",
    "# Note that this data only saves:\n",
    "# - residuals[:-1]\n",
    "# - embeds[1:]\n",
    "# since we only use each residual to predict the next embedding.\n",
    "# Thus they should match up already.\n",
    "\n",
    "def load_all_data(index: int = 0):\n",
    "    split_sections = load_dataset(\"nickypro/llama-3b-split\", split=\"train\")\n",
    "\n",
    "    res_data_file_path = hf_hub_download(\n",
    "        repo_id=\"nickypro/llama-3b-residuals\",\n",
    "        filename=f\"res_data_{index:03d}.pt\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    res_data = torch.load(res_data_file_path, map_location='cpu')\n",
    "\n",
    "    embeds_file_path = hf_hub_download(\n",
    "        repo_id=\"nickypro/llama-3b-embeds\",\n",
    "        filename=f\"embeds_{index:03d}.pt\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    embeds = torch.load(embeds_file_path, map_location='cpu')\n",
    "\n",
    "    assert len(res_data) == len(embeds)\n",
    "    dataset = []\n",
    "    res_reshape = \"1 layer section dim -> section layer dim\"\n",
    "    for i, (res, embed) in enumerate(zip(res_data, embeds)):\n",
    "        _id = i + 1000 * index\n",
    "        dataset.append({\n",
    "            \"id\": _id,\n",
    "            \"res_data\": einops.rearrange(res, res_reshape),\n",
    "            \"embeds\": embed,\n",
    "            \"split_text\": split_sections[_id][\"split\"],\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "dataset = load_all_data()\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset[0][\"id\"])\n",
    "print(dataset[0][\"res_data\"].shape)\n",
    "print(dataset[0][\"embeds\"].shape)\n",
    "[print([p]) for p in dataset[0][\"split_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c287f-7ef4-482c-8248-7bb31e46b226",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Ok now we have all the data we need. So we can start training, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e91da-64cc-45b3-afea-e97f74b165a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%# %% [markdown]\n",
    "\n",
    "# ## Exercise 3: Normalization and Preprocessing\n",
    "#\n",
    "# One issue with residual streams, is that the magnitudes of the activations can vary a lot between layers, often by orders of magnitude.\n",
    "# This can cause issues for training, so we need to normalize the data.\n",
    "# We use Welford's algorithm to compute running statistics.\n",
    "# We compute the mean and variance of the residual streams and embeddings, and then normalize the data to have mean 0 and variance 1.\n",
    "# We then store the mean and variance, so we can restore the data later.\n",
    "#\n",
    "# In essense, we do the most naive method of normalization, which is to look at each dimension independently and normalize it to have mean 0 and variance 1.\n",
    "# There may be better ways do do this, I have not spent much time optimizing this.\n",
    "# While we do need to normalize the residuals, I am not sure if we need to do it for the embeddings, but I do it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf0a45-e493-4862-8b64-928cfb3d9756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing normalization statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 756.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized residual mean: 0.0013, std: 1.8984\n",
      "Normalized embeds mean: 0.0130, std: 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "@dataclass\n",
    "class WelfordStats:\n",
    "    \"\"\"Track running mean and variance using Welford's algorithm.\"\"\"\n",
    "    mean: torch.Tensor\n",
    "    m2: torch.Tensor\n",
    "    count: int\n",
    "\n",
    "    def __init__(self, mean: torch.Tensor = None, m2: torch.Tensor = None, count: int = 0):\n",
    "        if mean is not None and m2 is not None:\n",
    "            self.mean = mean\n",
    "            self.m2 = m2\n",
    "            self.count = count\n",
    "        else:\n",
    "            self.mean = None\n",
    "            self.m2 = None\n",
    "            self.count = 0\n",
    "\n",
    "    def update(self, new_data: torch.Tensor):\n",
    "        \"\"\"Update statistics with new batch of data (batched version, true Welford).\"\"\"\n",
    "        # new_data: (batch, d)\n",
    "        if self.mean is None or self.m2 is None:\n",
    "            self.mean = torch.zeros_like(new_data[0])\n",
    "            self.m2 = torch.zeros_like(new_data[0])\n",
    "            self.count = 0\n",
    "        for x in new_data:\n",
    "            self.count += 1\n",
    "            delta = x - self.mean\n",
    "            self.mean += delta / self.count\n",
    "            delta2 = x - self.mean\n",
    "            self.m2 += delta * delta2\n",
    "\n",
    "    @property\n",
    "    def sample_variance(self):\n",
    "        # Unbiased sample variance\n",
    "        return self.m2 / (self.count - 1) if self.count > 1 else torch.zeros_like(self.m2)\n",
    "\n",
    "    @property\n",
    "    def population_variance(self):\n",
    "        # Population variance\n",
    "        return self.m2 / self.count if self.count > 0 else torch.zeros_like(self.m2)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        # Use sample variance by default\n",
    "        return torch.sqrt(self.sample_variance + 1e-6)\n",
    "\n",
    "class Normalizer:\n",
    "    \"\"\"Normalize data using precomputed statistics.\"\"\"\n",
    "    def __init__(self, mean: torch.Tensor, std: torch.Tensor, device: str = DEVICE):\n",
    "        self.mean = mean.to(device)\n",
    "        self.std = std.to(device)\n",
    "\n",
    "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "    def restore(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * (self.std + 1e-6) + self.mean\n",
    "\n",
    "# Compute normalization statistics (in practice, load precomputed stats)\n",
    "print(\"Computing normalization statistics...\")\n",
    "res_stats = WelfordStats()\n",
    "embed_stats = WelfordStats()\n",
    "\n",
    "# Update with data\n",
    "for i, example in enumerate(tqdm(dataset)):\n",
    "    res_stats.update(example[\"res_data\"])\n",
    "    embed_stats.update(example[\"embeds\"])\n",
    "\n",
    "# Create normalizers\n",
    "res_normalizer = Normalizer(res_stats.mean, res_stats.std, device='cpu')\n",
    "embed_normalizer = Normalizer(embed_stats.mean, embed_stats.std, device='cpu')\n",
    "\n",
    "# Test normalization\n",
    "def test_normalization(dataset):\n",
    "    normalized_res    = res_normalizer.normalize(dataset[0][\"res_data\"])\n",
    "    normalized_embeds = embed_normalizer.normalize(dataset[0][\"embeds\"])\n",
    "    print(f\"Normalized residual mean: {normalized_res.mean():.4f}, std: {normalized_res.std():.4f}\")\n",
    "    print(f\"Normalized embeds mean: {normalized_embeds.mean():.4f}, std: {normalized_embeds.std():.4f}\")\n",
    "\n",
    "test_normalization(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3eb83f-054a-4609-8b0a-5ecc9ff77906",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 4: Define Probe Models\n",
    "\n",
    " We'll implement a simple Linear probes to map from residual streams to SONAR embeddings.\n",
    " We could take all of the layers [0..57] but I found diminishing returns after 24 layers.\n",
    " I also have tried MLPs, but their performance was basically identical to the linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b95916-16c7-42ad-81d0-b1e47dc0aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear probe parameters: 75,498,496\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"Simple linear mapping from residual stream to SONAR embedding.\"\"\"\n",
    "    def __init__(self, d_res: int = 3072, d_sonar: int = 1024, n_layers_to_use: int = 24):\n",
    "        super().__init__()\n",
    "        self.n_layers_to_use = n_layers_to_use\n",
    "        d_in = d_res * self.n_layers_to_use\n",
    "        self.linear = nn.Linear(d_in, d_sonar)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # use the last n_layers_to_use layers of residual diffs\n",
    "        x = x[..., -self.n_layers_to_use:, :].flatten(start_dim=-2)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create probe models\n",
    "d_res = dataset[0][\"res_data\"].shape[-1]\n",
    "linear_probe = LinearProbe(d_res).to(DEVICE, DTYPE)\n",
    "\n",
    "print(f\"Linear probe parameters: {sum(p.numel() for p in linear_probe.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6c101-5399-4a77-a331-f4ee316cdb4a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Exercise 5: Training Loop\n",
    "\n",
    " We now train the probe to map from [residual stream] to [SONAR embedding].\n",
    " For efficiency, we currently load data from 10,000 texts (index=0...9), but this could be extended to 100,000 (index=0...99).\n",
    " We use index 99 as a relatively independent validation set, and validate every epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd57d7a-3bdb-447b-944f-856df102ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs\n",
      "Training files: 10, Validation files: 1\n",
      "Initial LR: 5e-05, LR Decay: 0.8\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 13.30s/it, Loss=1.2204, LR=5.00e-05]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.2204, Val Loss: 1.0875\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:49<00:00, 10.92s/it, Loss=0.9857, LR=4.00e-05]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.9857, Val Loss: 1.0227\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:14<00:49, 12.29s/it, Loss=0.9418, LR=3.20e-05]"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "class ProbeTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        probe: nn.Module,\n",
    "        lr: float = 1e-5,\n",
    "        weight_decay: float = 1e-6,\n",
    "        lr_decay: float = 0.8,\n",
    "        batch_size: int = 1024,\n",
    "        dtype=DTYPE,\n",
    "        device=DEVICE,\n",
    "        checkpoint_dir: str = \"./checkpoints\",\n",
    "    ):\n",
    "        self.probe = probe\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay = lr_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        # Training components\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.probe.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer,\n",
    "            step_size=1,\n",
    "            gamma=self.lr_decay\n",
    "        )\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_dataset(dataset: list[dict]):\n",
    "        \"\"\"Convert dataset to tensors and normalize.\"\"\"\n",
    "        dataset_dict = {\n",
    "            \"texts\": [example[\"split_text\"][1:] for example in dataset],\n",
    "            \"res_data\": res_normalizer.normalize(torch.cat([example[\"res_data\"] for example in dataset])),\n",
    "            \"embeds\": embed_normalizer.normalize(torch.cat([example[\"embeds\"] for example in dataset])),\n",
    "        }\n",
    "        return dataset_dict\n",
    "\n",
    "    def get_dataloader(self, res_data, embeds, shuffle=True):\n",
    "        \"\"\"Create DataLoader with proper memory management.\"\"\"\n",
    "        dataset = torch.utils.data.TensorDataset(res_data, embeds)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def train_epoch(self, epoch: int, train_indices: List[int]) -> float:\n",
    "        \"\"\"Train for one epoch with improved memory management.\"\"\"\n",
    "        self.probe.train()\n",
    "        epoch_train_loss = 0\n",
    "        n_train_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_indices, desc=f\"Train Epoch {epoch+1}\")\n",
    "        for data_idx in pbar:\n",
    "            try:\n",
    "                # Load data for this file\n",
    "                dataset = load_all_data(data_idx)\n",
    "                dataset_dict = self.preprocess_dataset(dataset)\n",
    "                res_data = dataset_dict[\"res_data\"]\n",
    "                embeds = dataset_dict[\"embeds\"].to(self.dtype)\n",
    "\n",
    "                # Create dataloader\n",
    "                loader = self.get_dataloader(res_data, embeds, shuffle=True)\n",
    "\n",
    "                # Training loop for this file\n",
    "                for batch_x, batch_y in loader:\n",
    "                    batch_x = batch_x.to(self.device, non_blocking=True)\n",
    "                    batch_y = batch_y.to(self.device, non_blocking=True)\n",
    "\n",
    "                    # Forward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    pred = self.probe(batch_x)\n",
    "                    loss = self.criterion(pred, batch_y)\n",
    "\n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_train_loss += loss.item()\n",
    "                    n_train_batches += 1\n",
    "\n",
    "                    # Update progress bar\n",
    "                    current_avg_loss = epoch_train_loss / n_train_batches\n",
    "                    pbar.set_postfix({\n",
    "                        \"Loss\": f\"{current_avg_loss:.4f}\",\n",
    "                        \"LR\": f\"{self.scheduler.get_last_lr()[0]:.2e}\"\n",
    "                    })\n",
    "\n",
    "                # Clean up memory after each file\n",
    "                del dataset, dataset_dict, res_data, embeds, loader\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {data_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return epoch_train_loss / max(n_train_batches, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_indices: List[int]) -> float:\n",
    "        \"\"\"Validate the model with improved memory management.\"\"\"\n",
    "        self.probe.eval()\n",
    "        epoch_val_loss = 0\n",
    "        n_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_idx in tqdm(val_indices, desc=\"Validation\"):\n",
    "                try:\n",
    "                    # Load validation data\n",
    "                    dataset = load_all_data(data_idx)\n",
    "                    dataset_dict = self.preprocess_dataset(dataset)\n",
    "                    res_data = dataset_dict[\"res_data\"]\n",
    "                    embeds = dataset_dict[\"embeds\"].to(self.dtype)\n",
    "\n",
    "                    # Create dataloader\n",
    "                    loader = self.get_dataloader(res_data, embeds, shuffle=False)\n",
    "\n",
    "                    # Validation loop for this file\n",
    "                    for batch_x, batch_y in loader:\n",
    "                        batch_x = batch_x.to(self.device, non_blocking=True)\n",
    "                        batch_y = batch_y.to(self.device, non_blocking=True)\n",
    "\n",
    "                        pred = self.probe(batch_x)\n",
    "                        loss = self.criterion(pred, batch_y)\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        n_val_batches += 1\n",
    "\n",
    "                    # Clean up memory after each file\n",
    "                    del dataset, dataset_dict, res_data, embeds, loader\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing validation file {data_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return epoch_val_loss / max(n_val_batches, 1)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_epochs: int = 1,\n",
    "        train_indices: List[int] = list(range(0, 99)),\n",
    "        val_indices: List[int] = [99],\n",
    "        save_checkpoints: bool = False,\n",
    "        validate_every: int = 1,\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Main training loop with improved features.\n",
    "\n",
    "        Args:\n",
    "            num_epochs: Number of epochs to train\n",
    "            train_indices: List of data file indices for training\n",
    "            val_indices: List of data file indices for validation\n",
    "            save_checkpoints: Whether to save model checkpoints\n",
    "            validate_every: Validate every N epochs\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing training and validation losses\n",
    "        \"\"\"\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        print(f\"Starting training for {num_epochs} epochs\")\n",
    "        print(f\"Training files: {len(train_indices)}, Validation files: {len(val_indices)}\")\n",
    "        print(f\"Initial LR: {self.lr}, LR Decay: {self.lr_decay}\")\n",
    "\n",
    "        try:\n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "                # Training\n",
    "                train_loss = self.train_epoch(epoch, train_indices)\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                # Validation\n",
    "                if epoch % validate_every == 0 or epoch == num_epochs - 1:\n",
    "                    val_loss = self.validate(val_indices)\n",
    "                    val_losses.append(val_loss)\n",
    "\n",
    "                    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "                    # Save checkpoint\n",
    "                    if save_checkpoints:\n",
    "                        checkpoint_path = os.path.join(\n",
    "                            self.checkpoint_dir,\n",
    "                            f\"probe_epoch_{epoch+1}.pkl\"\n",
    "                        )\n",
    "                        self.save_checkpoint(checkpoint_path, epoch, train_loss, val_loss)\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "                # Step the learning rate scheduler\n",
    "                self.scheduler.step()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nTraining error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            torch.set_grad_enabled(False)\n",
    "\n",
    "        return {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses\n",
    "        }\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_path: str, epoch: int, train_loss: float, val_loss: float):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.probe.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.probe.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return checkpoint\n",
    "\n",
    "linear_probe = LinearProbe(d_res).to(DEVICE, DTYPE)\n",
    "\n",
    "# Use the improved ProbeTrainer\n",
    "trainer = ProbeTrainer(\n",
    "    probe=linear_probe,\n",
    "    lr=5e-5,\n",
    "    lr_decay=0.8,\n",
    "    batch_size=1024,\n",
    "    checkpoint_dir=\"./probe_checkpoints\"\n",
    ")\n",
    "\n",
    "# Train with improved features\n",
    "losses = trainer.train(\n",
    "    num_epochs=10,\n",
    "    train_indices=list(range(0, 10)),  # Reduced for demo\n",
    "    val_indices=[99],\n",
    "    save_checkpoints=True,\n",
    "    validate_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16f073-9247-41de-8600-ca1d901092e9",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Test performance of the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75b609-4486-4728-974d-fe7b3a29a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_name = lambda x: [k for k,v in globals().items() if v is x][0]\n",
    "    print(f\"{get_name(vec2text)} already loaded\")\n",
    "except:\n",
    "    vec2text = EmbeddingToTextModelPipeline(\n",
    "        decoder=\"text_sonar_basic_decoder\",\n",
    "        tokenizer=\"text_sonar_basic_encoder\",\n",
    "        device=DEVICE,\n",
    "        dtype=DTYPE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05723bb4-1e58-4655-b88a-98851c4bcc6b",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " You should see that some predictions are pretty similar to the original text.\n",
    " And some of them are completelly broken tbh.\n",
    " You can get better results if you increase train_indices to be range(0, 99), but this will take a while."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
