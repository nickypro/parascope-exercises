{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013d4743-86be-47ca-b988-6016a556fe5e",
   "metadata": {},
   "source": [
    " # Section 1: Text Autoencoders - Exploring SONAR\n",
    " This notebook explores Meta's SONAR text autoencoder, which can encode text\n",
    " into fixed-size vectors and decode them back to (approximately) the original text.\n",
    " Learning objectives:\n",
    " 1. Load and use SONAR for text encoding/decoding\n",
    " 2. Understand the properties of text embeddings\n",
    " 3. Test robustness to noise\n",
    " 4. Explore how text length affects embeddings\n",
    " 5. Experiment with token swapping and sentence combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a8bb5-ea0d-43c0-a75e-371908c60fe7",
   "metadata": {},
   "source": [
    " ## Setup and Installation\n",
    "\n",
    " First, we need to install SONAR and its dependencies. Just run, nothing worth reading here unless you get errors.\n",
    " Note: You may need to adjust the CUDA version in fairseq2 installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a0021-b8a2-4b04-9fef-cfe887a422b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "!pip install -q fairseq2==0.4.5 sonar-space==0.4.0 torchvision==0.21.0 torch==2.6.0 torchaudio==2.6.0 plotly nbformat\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from jaxtyping import Float\n",
    "\n",
    "# Check if CUDA is available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device(DEVICE)\n",
    "torch.set_grad_enabled(False)  # We're only doing inference\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384629e6-ecb0-4604-a887-6238065eb1ea",
   "metadata": {},
   "source": [
    " ## Loading SONAR Models\n",
    "\n",
    " SONAR (Sentence-Level Multimodal and Language-Agnostic Representations) is Meta's text autoencoder\n",
    " that can encode entire sentences/paragraphs into fixed-size vectors and decode them back to approximately\n",
    " the original text.\n",
    "\n",
    " **What are Text Autoencoders?**\n",
    "\n",
    " Text Autoencoders are models that compress entire input sequences (sentences/paragraphs) into a single\n",
    " fixed-size vector representation (the \"bottleneck\"), then reconstruct the original text from that vector.\n",
    " Unlike typical text embedding models that only encode, these models have both an encoder AND decoder.\n",
    "\n",
    " ![Text Autoencoder Architecture](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db8d350884974ce6dcb1281011c5053e11b65711c12a4556.png)\n",
    "\n",
    " **How Text Autoencoders Work:**\n",
    " 1. **Encoder**: Takes input text → processes through Transformer → outputs single fixed-size vector (1024-dim)\n",
    " 2. **Bottleneck**: The compressed representation that captures semantic meaning in a dense vector\n",
    " 3. **Decoder**: Takes the vector → generates text that approximates the original input\n",
    "\n",
    " **Key Properties:**\n",
    " - **Lossy compression**: Some information is lost, but semantic meaning is preserved\n",
    " - **Fixed-size representation**: Any length text becomes same-size vector (useful for comparison/clustering)\n",
    " - **Cross-lingual**: Can encode in one language and decode in another\n",
    " - **Reconstruction capability**: Unlike embedding-only models, you can decode back to text\n",
    " - **Semantic preservation**: The bottleneck captures core meaning even with compression\n",
    "\n",
    " **SONAR Specifically:**\n",
    " - Trained on ~100B tokens with denoising and translation objectives\n",
    " - Uses 24-layer Transformer encoder and decoder, with mean-pooling to create the bottleneck vector\n",
    " - Supports 200+ languages and can handle up to 512 tokens of context\n",
    " - Currently one of the best-performing text autoencoders available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdddcbd-abb0-4e1a-b991-04f8e10f356a",
   "metadata": {},
   "source": [
    " We start by loading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb337a-3892-41b1-b0a2-879ed94058fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading SONAR models...\")\n",
    "text2vec = TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=DEVICE\n",
    ")\n",
    "vec2text = EmbeddingToTextModelPipeline(\n",
    "    decoder=\"text_sonar_basic_decoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=DEVICE\n",
    ")\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a5ace-16a9-4d29-bf91-9c3c1a71b2f7",
   "metadata": {},
   "source": [
    " ## Basic Usage - Encoding and Decoding\n",
    "\n",
    " Test basic encoding and decoding functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db69055-63ae-4c68-acb7-c15255ed8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Simple example sentences\n",
    "sentences = [\n",
    "    'My name is SONAR.',\n",
    "    'I can embed sentences into vectorial space.'\n",
    "]\n",
    "\n",
    "# Encode sentences to vectors\n",
    "embeddings = text2vec.predict(sentences, source_lang=\"eng_Latn\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # Should be [2, 1024]\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"L2 norm of embeddings: {torch.norm(embeddings, dim=1).tolist()}\")\n",
    "\n",
    "# Decode vectors back to text\n",
    "reconstructed = vec2text.predict(embeddings, target_lang=\"eng_Latn\", max_seq_len=512)\n",
    "print(\"\\nReconstruction quality:\")\n",
    "for orig, rec in zip(sentences, reconstructed):\n",
    "    print(f\"Original:      {orig}\")\n",
    "    print(f\"Reconstructed: {rec}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac4423-350f-4570-9ca1-43a9c65b50a3",
   "metadata": {},
   "source": [
    " ## Exercise 1: Testing with Longer, More Realistic Text\n",
    " Let's test how well SONAR handles paragraph-length text.\n",
    "\n",
    " Write a function to reconstruct text from SONAR embeddings, and try testing with some longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b90870-b7a8-4326-8976-a3a6134ce62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(texts: list[str]) -> list[str]:\n",
    "    \"\"\"Reconstruct text from SONAR embedding, by first encoding and then decoding the text.\n",
    "\n",
    "    Args:\n",
    "        texts: List of strings to embed and then reconstruct.\n",
    "\n",
    "    Returns:\n",
    "        List of reconstructed strings.\n",
    "    \"\"\"\n",
    "    # [your implementation here]\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Longer example paragraphs\n",
    "paragraph1 = \"\"\"SONAR is a model from August 2023, trained as a semantic text auto-encoder,\n",
    "converting text into semantic embed vectors, which can later be decoded back into text.\n",
    "Additionally, the model is trained such that the semantic embed vectors are to some degree\n",
    "\"universal\" for different languages, and one can embed in French and decode in English.\"\"\"\n",
    "\n",
    "paragraph2 = \"\"\"I tried it, and SONAR seems to work surprisingly well. For example, the above\n",
    "paragraph and this paragraph, if each are encoded into two 1024 dimensional vectors\n",
    "(one for each paragraph), the model returns the following decoded outputs.\"\"\"\n",
    "\n",
    "paragraph3 = \"\"\"\\\n",
    "Your text here.\n",
    "\"\"\"\n",
    "\n",
    "# Test with paragraphs\n",
    "long_texts = [paragraph1, paragraph2, paragraph3]\n",
    "long_reconstructed = reconstruct_text(long_texts)\n",
    "\n",
    "print(\"Paragraph reconstruction:\")\n",
    "for i, (orig, rec) in enumerate(zip(long_texts, long_reconstructed)):\n",
    "    print(f\"\\n--- Paragraph {i+1} ---\")\n",
    "    print(f\"Original ({len(orig)} chars):\")\n",
    "    print(orig[:100] + \"...\" if len(orig) > 100 else orig)\n",
    "    print(f\"\\nReconstructed ({len(rec)} chars):\")\n",
    "    print(rec[:100] + \"...\" if len(rec) > 100 else rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ab20a-ed82-4666-a014-d4d3d941ce63",
   "metadata": {},
   "source": [
    " How well does it work for longer text? It should be doing a pretty good job. Bonus: How long does the text get before you see some degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125bf55-1adb-4f05-a051-4a21fe46d05d",
   "metadata": {},
   "source": [
    " ## Exercise 2: Noise Robustness Analysis\n",
    "\n",
    " In this exercise, we investigate SONAR's robustness to perturbations in the embedding space.\n",
    " We'll systematically add Gaussian noise of increasing magnitude to text embeddings and analyze\n",
    " how reconstruction quality degrades. This helps us understand:\n",
    " 1. How stable the embedding space is to small perturbations\n",
    " 2. The sensitivity of the decoder to different noise directions\n",
    "\n",
    " Write a function to test the robustness of SONAR to noise, and try it out with some different noise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efa499-ea13-4bfc-b42c-baf054b697de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_noise_robustness(text, noise_levels):\n",
    "    \"\"\"Test how reconstruction quality degrades with noise.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get original embedding\n",
    "    original_emb = text2vec.predict([text], source_lang=\"eng_Latn\")\n",
    "    original_norm = torch.norm(original_emb)\n",
    "\n",
    "    results = []\n",
    "    for noise_scale in noise_levels:\n",
    "        # [your implementation here]\n",
    "        # Add Gaussian noise\n",
    "        # Decode noisy embedding\n",
    "        # Calculate cosine similarity\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        results.append({\n",
    "            'noise_scale': noise_scale,\n",
    "            'cosine_similarity': cosine_sim,\n",
    "            'reconstruction': reconstructed\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test with different noise levels\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "noise_levels = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "\n",
    "print(f\"Original text: {test_text}\\n\")\n",
    "results = test_noise_robustness(test_text, noise_levels)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Noise scale: {res['noise_scale']:.1f}\")\n",
    "    print(f\"Cosine similarity: {res['cosine_similarity']:.3f}\")\n",
    "    print(f\"Reconstructed: {res['reconstruction']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82998d9-a22d-477f-bf75-48516572fe80",
   "metadata": {},
   "source": [
    " What do you see?\n",
    " It should be the case that with little noise, the reconstruction is still good. With more noise, the reconstruction gets worse. However, I found there is a lot of variance in the results, so try running it a few times. It seems like some directions have basically no effect, and others have a lot of effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf9b01-0df6-4dee-a998-5dba2fd1be97",
   "metadata": {},
   "source": [
    " ## Exercise 3: Text Length vs Vector Norm Analysis\n",
    "\n",
    " ### Exercise 3: Investigating the Relationship Between Text Length and Embedding Norms\n",
    "\n",
    " In this exercise, we'll explore whether there's a correlation between the length of text\n",
    " and the L2 norm (magnitude) of its embedding vector. This analysis will help us understand:\n",
    " - How semantic information is distributed across embedding dimensions\n",
    " - Whether longer texts result in larger embedding magnitudes\n",
    " - If the embedding space has inherent biases based on text length\n",
    "\n",
    " We'll test this hypothesis using three different types of text:\n",
    " 1. Repeated words (to test pure length effects)\n",
    " 2. Random character sequences (to test meaningless content)\n",
    " 3. Natural language sentences (to test realistic content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473836e8-8f4e-4466-a814-7f9f085a3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Collect all data first\n",
    "data = []\n",
    "def add_data(text, text_type):\n",
    "    emb = text2vec.predict([text], source_lang=\"eng_Latn\")\n",
    "    norm = torch.norm(emb).item()\n",
    "    data.append({\n",
    "        'text': text,\n",
    "        'length': len(text),\n",
    "        'norm': norm,\n",
    "        'type': text_type\n",
    "    })\n",
    "\n",
    "# Repeated words (more examples)\n",
    "for length in range(1, 100):\n",
    "    for word in ['word', 'sentence', 'paragraph', 'dog', 'spicy', 'anime']:\n",
    "        words = [word] * length\n",
    "        text = ' '.join(words)\n",
    "        add_data(text, 'Repeated Words')\n",
    "\n",
    "# Random characters (more examples)\n",
    "random.seed(42)\n",
    "for length in range(1, 100, ):\n",
    "        random_words = [''.join(random.choices(string.ascii_lowercase, k=random.randint(3, 8))) for _ in range(length)]\n",
    "        text = ' '.join(random_words)\n",
    "        add_data(text, 'Random Characters')\n",
    "\n",
    "# Normal sentences (many more examples)\n",
    "normal_sentences = [\n",
    "    \"Hi\",\n",
    "    \"Hello\",\n",
    "    \"Good morning\",\n",
    "    \"Hello there\",\n",
    "    \"How are you?\",\n",
    "    \"Nice to meet you\",\n",
    "    \"The cat sat on the mat\",\n",
    "    \"I like to read books\",\n",
    "    \"The weather is nice today\",\n",
    "    \"She went to the store yesterday\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I enjoy listening to music in the evening\",\n",
    "    \"She sells seashells by the seashore on weekends\",\n",
    "    \"To be or not to be, that is the question\",\n",
    "    \"The early bird catches the worm every morning\",\n",
    "    \"A picture is worth a thousand words in most cases\"\n",
    "]\n",
    "for text in normal_sentences:\n",
    "    add_data(text, 'Real Text')\n",
    "\n",
    "# Load dataset of some example texts generated by Llama3b\n",
    "dataset = load_dataset(\"nickypro/fineweb-llama3b-regen-split\", split=\"train\")\n",
    "for split_text in dataset.select(range(20)):\n",
    "    for paragraph in split_text['split_text']:\n",
    "        add_data(paragraph, 'Real Text')\n",
    "\n",
    "\n",
    "# Create DataFrame and plot\n",
    "df = pd.DataFrame(data)\n",
    "# Truncate text to first 50 characters for hover display\n",
    "df['text_truncated'] = df['text'].str[:50] + '...'\n",
    "fig = px.scatter(df,\n",
    "        x='length', y='norm', color='type',\n",
    "        title=\"Text Length vs Embedding Norm\",\n",
    "        labels={'length': 'Text Length (characters)', 'norm': 'Embedding L2 Norm'},\n",
    "        hover_data=['text_truncated'],\n",
    "        opacity=0.5,\n",
    "        log_x=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2568fc-4231-428b-a042-96b3f40c8a49",
   "metadata": {},
   "source": [
    " ## Exercise 4: Token Swapping Experiments\n",
    "\n",
    " This exercise explores how we can manipulate text embeddings to perform token swapping.\n",
    " We'll investigate:\n",
    " 1. Building difference vectors between similar texts\n",
    " 2. Applying global transformations to swap words\n",
    " 3. Creating position-specific transformations for targeted edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8774cb6-d519-426f-b59a-7c7cc551129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def diff_vector(src_text: str, tgt_text: str) -> Float[torch.Tensor, \"1024\"]:\n",
    "    \"\"\"Return embedding difference between *tgt_text* and *src_text* (tgt − src).\"\"\"\n",
    "    # [your implementation here]\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def decode(embedding: torch.Tensor, max_seq_len: int = 512) -> str:\n",
    "    \"\"\"Greedy‑decode a single 1024‑D embedding back to text.\"\"\"\n",
    "    return vec2text.predict(embedding.unsqueeze(0), target_lang=\"eng_Latn\", max_seq_len=max_seq_len)[0]\n",
    "\n",
    "\n",
    "def positional_diff(src_word: str, tgt_word: str, pos: int, *, seq_len: int, filler: str = \"_\") -> torch.Tensor:\n",
    "    \"\"\"Build a difference vector that swaps **src_word→tgt_word** at index *pos*.\n",
    "\n",
    "    All other positions are filled with *filler* tokens so that the vector is\n",
    "    specific to that location.\n",
    "    \"\"\"\n",
    "    # [your implementation here]\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert diff_vector(\"dog\", \"cat\").shape == (1024,)\n",
    "assert isinstance(decode(torch.randn(1024), 5), str)\n",
    "assert positional_diff(\"dog\", \"cat\", pos=1, seq_len=8, filler=\"a\").shape == (1024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c0c62-1be9-484e-a17b-9106326897f5",
   "metadata": {},
   "source": [
    " Now we can try see what the difference vector does in different cases.\n",
    " 1. Global dog→cat vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bfcf4-3078-4d77-be56-7e23db3385f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. Global word swapping:\")\n",
    "swap_vec = diff_vector(\"dog\", \"cat\")\n",
    "sentence = \"the dog is happy in the dog house\"\n",
    "sent_emb = text2vec.predict([sentence], source_lang=\"eng_Latn\").squeeze(0)\n",
    "\n",
    "print(f\"Original:               {decode(sent_emb)}\")\n",
    "print(f\"Global swap dog→cat:    {decode(sent_emb + swap_vec)}\")\n",
    "\n",
    "# 2. Position‑specific swap\n",
    "print(\"\\n2. Position-specific swapping:\")\n",
    "# Swap only the token at index 1 (0‑based) in a sentence\n",
    "pos_vec = positional_diff(\"dog\", \"cat\", pos=1, seq_len=8, filler=\"a\")\n",
    "print(f\"Position‑aware swap:    {decode(sent_emb + pos_vec)}\")\n",
    "\n",
    "# 3. Test with different word pairs\n",
    "print(\"\\n3. Testing different word pairs:\")\n",
    "word_pairs = [(\"happy\", \"sad\"), (\"house\", \"tree\"), (\"big\", \"small\")]\n",
    "for src, tgt in word_pairs:\n",
    "    swap_vec = diff_vector(src, tgt)\n",
    "    test_sentence = f\"the {src} animal lives here\"\n",
    "    test_emb = text2vec.predict([test_sentence], source_lang=\"eng_Latn\").squeeze(0)\n",
    "    print(f\"{src}→{tgt}: '{test_sentence}' → '{decode(test_emb + swap_vec)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dcc35-b27d-40f1-999e-9319673792ec",
   "metadata": {},
   "source": [
    " ## Exercise 5: Sentence Combination\n",
    "\n",
    " This exercise explores how we can combine two sentences into a single embedding.\n",
    " So far I have only tried a couple of the most naive approaches. It's ok but I suspect it should be easy to try better approaches to this also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78613cb9-461b-4bba-82e0-039bfaa3f589",
   "metadata": {},
   "source": [
    " ### Part 1: Basic Combination Analysis\n",
    "\n",
    " First, let's analyze how SONAR combines sentences with different relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9a589-e080-434a-89de-52b451a28d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Create diverse sentence pairs for analysis\n",
    "sentence_pairs = [\n",
    "    # Related sentences (continuation)\n",
    "    (\"The weather is beautiful today\", \"I think I'll go for a walk\"),\n",
    "    (\"She opened the mysterious letter\", \"Her hands trembled as she read it\"),\n",
    "\n",
    "    # Contrasting sentences\n",
    "    (\"I love sunny days\", \"But I hate the rain\"),\n",
    "    (\"The movie was exciting\", \"However, the ending disappointed me\"),\n",
    "\n",
    "    # Unrelated sentences\n",
    "    (\"Cats are independent animals\", \"Python is a programming language\"),\n",
    "    (\"The Earth orbits the Sun\", \"Pizza is my favorite food\"),\n",
    "\n",
    "    # Question-answer pairs\n",
    "    (\"What's yraise NotImplementedError()our favorite color?\", \"My favorite color is blue\"),\n",
    "    (\"Where do you live?\", \"I live in New York City\"),\n",
    "]\n",
    "\n",
    "# Analyze combinations\n",
    "combination_data = []\n",
    "for sent_a, sent_b in sentence_pairs:\n",
    "    # Individual embeddings\n",
    "    emb_a = text2vec.predict([sent_a], source_lang=\"eng_Latn\")\n",
    "    emb_b = text2vec.predict([sent_b], source_lang=\"eng_Latn\")\n",
    "\n",
    "    # Combined embeddings (both orders)\n",
    "    combined_ab = f\"{sent_a} {sent_b}\"\n",
    "    combined_ba = f\"{sent_b} {sent_a}\"\n",
    "    emb_ab = text2vec.predict([combined_ab], source_lang=\"eng_Latn\")\n",
    "    emb_ba = text2vec.predict([combined_ba], source_lang=\"eng_Latn\")\n",
    "\n",
    "    # Various combinations\n",
    "    emb_avg = (emb_a + emb_b) / 2\n",
    "    emb_sum = emb_a + emb_b\n",
    "    emb_diff = emb_a - emb_b\n",
    "\n",
    "    # Calculate similarities\n",
    "    data = {\n",
    "        'sent_a': sent_a[:30] + '...' if len(sent_a) > 30 else sent_a,\n",
    "        'sent_b': sent_b[:30] + '...' if len(sent_b) > 30 else sent_b,\n",
    "        'sim_ab_a': torch.nn.functional.cosine_similarity(emb_ab, emb_a).item(),\n",
    "        'sim_ab_b': torch.nn.functional.cosine_similarity(emb_ab, emb_b).item(),\n",
    "        'sim_ab_ba': torch.nn.functional.cosine_similarity(emb_ab, emb_ba).item(),\n",
    "        'sim_ab_avg': torch.nn.functional.cosine_similarity(emb_ab, emb_avg).item(),\n",
    "        'sim_ab_sum': torch.nn.functional.cosine_similarity(emb_ab, emb_sum).item(),\n",
    "        'order_sensitivity': torch.norm(emb_ab - emb_ba).item()\n",
    "    }\n",
    "    combination_data.append(data)\n",
    "\n",
    "# Display results\n",
    "df_comb = pd.DataFrame(combination_data)\n",
    "print(\"Sentence Combination Analysis:\")\n",
    "print(df_comb.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d4592-0d69-48a6-a77b-af87c3701780",
   "metadata": {},
   "source": [
    " ### Part 2: Try simple linear combination\n",
    " If we want to combine two sentences, we can just add their embeddings? Or maybe average them? Will this give us something that works as an embedding with two sentences side-by-side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61ffde-b6ad-4dcf-afa3-70e538d615e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class SimpleLinearCombiner(nn.Module):\n",
    "    def __init__(self, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "basic_combiner_model = SimpleLinearCombiner().to(DEVICE)\n",
    "\n",
    "# Test the simple linear combiner\n",
    "def test_performance_on_new_examples(model, verbose=True):\n",
    "    \"\"\"Test model performance on predefined pairs plus one random example\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Predefined test pairs\n",
    "    test_pairs = [\n",
    "        (\"It started raining heavily.\", \"Everyone ran for shelter.\"),\n",
    "        (\"First, preheat the oven.\", \"Then, mix the ingredients.\"),\n",
    "        (\"The book was fascinating.\", \"The movie adaptation was terrible.\"),\n",
    "        (\"I need to buy milk.\", \"I also need to get bread.\"),\n",
    "    ]\n",
    "\n",
    "    # Add one random pair\n",
    "    # idx1, idx2 = np.random.choice(len(all_sentences), 2, replace=False)\n",
    "    # test_pairs.append((all_sentences[idx1], all_sentences[idx2]))\n",
    "\n",
    "    test_results = []\n",
    "\n",
    "    for sent1, sent2 in test_pairs:\n",
    "        # Get embeddings\n",
    "        emb1 = text2vec.predict([sent1], source_lang=\"eng_Latn\").to(DEVICE)\n",
    "        emb2 = text2vec.predict([sent2], source_lang=\"eng_Latn\").to(DEVICE)\n",
    "        emb_true = text2vec.predict([f\"{sent1} {sent2}\"], source_lang=\"eng_Latn\").to(DEVICE)\n",
    "\n",
    "        # Predict and decode\n",
    "        with torch.no_grad():\n",
    "            emb_pred = model(emb1.squeeze(0), emb2.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        text_true = vec2text.predict(emb_true.cpu(), target_lang=\"eng_Latn\")[0]\n",
    "        text_pred = vec2text.predict(emb_pred.cpu(), target_lang=\"eng_Latn\")[0]\n",
    "        similarity = torch.cosine_similarity(emb_pred, emb_true, dim=-1).item()\n",
    "\n",
    "        test_results.append({\n",
    "            'sent1': sent1, 'sent2': sent2, 'decoded_true': text_true,\n",
    "            'decoded_pred': text_pred, 'similarity': similarity\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nSent1: {sent1}\")\n",
    "            print(f\"Sent2: {sent2}\")\n",
    "            print(f\"True: {text_true}\")\n",
    "            print(f\"Pred: {text_pred}\")\n",
    "            print(f\"Similarity: {similarity:.4f}\")\n",
    "\n",
    "    avg_similarity = np.mean([r['similarity'] for r in test_results])\n",
    "    print(f\"\\nAverage similarity: {avg_similarity:.4f}\")\n",
    "    return test_results\n",
    "\n",
    "# Test the simple linear combiner\n",
    "test_results = test_performance_on_new_examples(basic_combiner_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
